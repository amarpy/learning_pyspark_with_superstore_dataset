{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_excel(\"superstore.xls\")\n",
    "superstore = spark.createDataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Row ID: long (nullable = true)\n",
      " |-- Order ID: string (nullable = true)\n",
      " |-- Order Date: timestamp (nullable = true)\n",
      " |-- Ship Date: timestamp (nullable = true)\n",
      " |-- Ship Mode: string (nullable = true)\n",
      " |-- Customer ID: string (nullable = true)\n",
      " |-- Customer Name: string (nullable = true)\n",
      " |-- Segment: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Postal Code: double (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- Product ID: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Sub-Category: string (nullable = true)\n",
      " |-- Product Name: string (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      " |-- Quantity: long (nullable = true)\n",
      " |-- Discount: double (nullable = true)\n",
      " |-- Profit: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df.printSchema() shows the schema\n",
    "\n",
    "superstore.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|            id|\n",
      "+--------------+\n",
      "|CA-2017-152156|\n",
      "|CA-2017-152156|\n",
      "|CA-2017-138688|\n",
      "|US-2016-108966|\n",
      "|US-2016-108966|\n",
      "+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df.selectExpr(\"`col name` as id\") gives a df with col name renamed in display\n",
    "\n",
    "superstore.selectExpr(\"`Order ID` as id\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+--------+\n",
      "|      Order ID|Postal Code|Quantity|\n",
      "+--------------+-----------+--------+\n",
      "|CA-2015-115259|    43229.0|      14|\n",
      "|CA-2017-145583|    95661.0|      14|\n",
      "|CA-2017-114489|    53132.0|      11|\n",
      "|CA-2017-145625|    92037.0|      13|\n",
      "|CA-2015-122336|    19140.0|      13|\n",
      "+--------------+-----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df.where(\"filter condition\") is used to filter the data based on a given logical condition\n",
    "\n",
    "superstore.where(\"Quantity > 10\")\\\n",
    "          .selectExpr(\"`Order ID`\", \"`Postal Code`\", \"`Quantity`\")\\\n",
    "          .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+--------+\n",
      "|      Order ID|Postal Code|Quantity|\n",
      "+--------------+-----------+--------+\n",
      "|CA-2016-104241|    22304.0|      14|\n",
      "|CA-2015-120768|    35630.0|      14|\n",
      "|CA-2015-163447|    10011.0|      14|\n",
      "|CA-2018-152702|    61107.0|      14|\n",
      "|CA-2017-145583|    95661.0|      14|\n",
      "+--------------+-----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import column\n",
    "\n",
    "#df.sort(col(\"col name\")).desc() orders the dataframe in descending order\n",
    "\n",
    "superstore.where(column(\"Category\") == \"Furniture\")\\\n",
    "          .select(\"Order ID\", \"Postal Code\", \"Quantity\")\\\n",
    "          .sort(column(\"Quantity\").desc())\\\n",
    "          .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+--------+\n",
      "|       Category|Region|Quantity|\n",
      "+---------------+------+--------+\n",
      "|Office Supplies|  West|       2|\n",
      "|Office Supplies|  West|       4|\n",
      "|     Technology|  West|       6|\n",
      "|Office Supplies|  West|       3|\n",
      "|Office Supplies|  West|       5|\n",
      "+---------------+------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#multiple df.where fucntions can be used to filter more conditions\n",
    "\n",
    "superstore.where(column(\"Category\") != \"Furniture\")\\\n",
    "          .where(column(\"Region\") == \"West\")\\\n",
    "          .selectExpr(\"Category\", \"Region\", \"Quantity\")\\\n",
    "          .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+-------+\n",
      "|     col_name|data_type|comment|\n",
      "+-------------+---------+-------+\n",
      "|       Row ID|   bigint|   null|\n",
      "|     Order ID|   string|   null|\n",
      "|   Order Date|timestamp|   null|\n",
      "|    Ship Date|timestamp|   null|\n",
      "|    Ship Mode|   string|   null|\n",
      "|  Customer ID|   string|   null|\n",
      "|Customer Name|   string|   null|\n",
      "|      Segment|   string|   null|\n",
      "|      Country|   string|   null|\n",
      "|         City|   string|   null|\n",
      "|        State|   string|   null|\n",
      "|  Postal Code|   double|   null|\n",
      "|       Region|   string|   null|\n",
      "|   Product ID|   string|   null|\n",
      "|     Category|   string|   null|\n",
      "| Sub-Category|   string|   null|\n",
      "| Product Name|   string|   null|\n",
      "|        Sales|   double|   null|\n",
      "|     Quantity|   bigint|   null|\n",
      "|     Discount|   double|   null|\n",
      "|       Profit|   double|   null|\n",
      "+-------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df.createOrReplaceTempView(\"view name\") can be used to create temp view which can be used to query tables using spark.sql\n",
    "\n",
    "superstore.createOrReplaceTempView(\"superstore_view\")\n",
    "\n",
    "spark.sql(\"\"\"DESCRIBE FORMATTED superstore_view\"\"\").show(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+--------+\n",
      "|       Category|Region|Quantity|\n",
      "+---------------+------+--------+\n",
      "|      Furniture| South|       2|\n",
      "|      Furniture| South|       3|\n",
      "|Office Supplies|  West|       2|\n",
      "|      Furniture| South|       5|\n",
      "|Office Supplies| South|       2|\n",
      "+---------------+------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#spark.sql(\"SQL QUERY\") can be used to query temp views\n",
    "\n",
    "spark.sql(\"SELECT Category, Region, Quantity FROM superstore_view\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Category: string, Region: string, sum(Quantity): bigint]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT  Category, Region, SUM(Quantity)\n",
    "FROM superstore_view \n",
    "GROUP BY Category, Region\n",
    "ORDER BY SUM(Quantity) DESC LIMIT 10\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+\n",
      "|  Category|test|\n",
      "+----------+----+\n",
      "| Furniture|true|\n",
      "| Furniture|true|\n",
      "| Furniture|true|\n",
      "|Technology|true|\n",
      "|Technology|true|\n",
      "+----------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" SELECT Category, (Region = \"Furniture\" AND Quantity > 10 OR SALES > 1000) as test\n",
    "FROM superstore_view WHERE Region = \"Furniture\" AND Quantity > 10 OR SALES > 1000\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>summary</th>\n",
       "      <td>count</td>\n",
       "      <td>mean</td>\n",
       "      <td>stddev</td>\n",
       "      <td>min</td>\n",
       "      <td>max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Row ID</th>\n",
       "      <td>9994</td>\n",
       "      <td>4997.5</td>\n",
       "      <td>2885.1636290974325</td>\n",
       "      <td>1</td>\n",
       "      <td>9994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Order ID</th>\n",
       "      <td>9994</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>CA-2015-100006</td>\n",
       "      <td>US-2018-169551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ship Mode</th>\n",
       "      <td>9994</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>First Class</td>\n",
       "      <td>Standard Class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Customer ID</th>\n",
       "      <td>9994</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AA-10315</td>\n",
       "      <td>ZD-21925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Customer Name</th>\n",
       "      <td>9994</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Aaron Bergman</td>\n",
       "      <td>Zuschuss Donatelli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Segment</th>\n",
       "      <td>9994</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>Home Office</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Country</th>\n",
       "      <td>9994</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>United States</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>City</th>\n",
       "      <td>9994</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Aberdeen</td>\n",
       "      <td>Yuma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State</th>\n",
       "      <td>9994</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Wyoming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Postal Code</th>\n",
       "      <td>9994</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Region</th>\n",
       "      <td>9994</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Central</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Product ID</th>\n",
       "      <td>9994</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>FUR-BO-10000112</td>\n",
       "      <td>TEC-PH-10004977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category</th>\n",
       "      <td>9994</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sub-Category</th>\n",
       "      <td>9994</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>Tables</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Product Name</th>\n",
       "      <td>9994</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>\"While you Were Out\" Message Book, One Form pe...</td>\n",
       "      <td>netTALK DUO VoIP Telephone Service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sales</th>\n",
       "      <td>9994</td>\n",
       "      <td>229.85800083049847</td>\n",
       "      <td>623.245100508681</td>\n",
       "      <td>0.44399999999999995</td>\n",
       "      <td>22638.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Quantity</th>\n",
       "      <td>9994</td>\n",
       "      <td>3.789573744246548</td>\n",
       "      <td>2.2251096911414012</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Discount</th>\n",
       "      <td>9994</td>\n",
       "      <td>0.15620272163297735</td>\n",
       "      <td>0.20645196782571612</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Profit</th>\n",
       "      <td>9994</td>\n",
       "      <td>28.6568963077847</td>\n",
       "      <td>234.2601076909574</td>\n",
       "      <td>-6599.978000000001</td>\n",
       "      <td>8399.975999999999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0                    1                    2  \\\n",
       "summary        count                 mean               stddev   \n",
       "Row ID          9994               4997.5   2885.1636290974325   \n",
       "Order ID        9994                 None                 None   \n",
       "Ship Mode       9994                 None                 None   \n",
       "Customer ID     9994                 None                 None   \n",
       "Customer Name   9994                 None                 None   \n",
       "Segment         9994                 None                 None   \n",
       "Country         9994                 None                 None   \n",
       "City            9994                 None                 None   \n",
       "State           9994                 None                 None   \n",
       "Postal Code     9994                  NaN                  NaN   \n",
       "Region          9994                 None                 None   \n",
       "Product ID      9994                 None                 None   \n",
       "Category        9994                 None                 None   \n",
       "Sub-Category    9994                 None                 None   \n",
       "Product Name    9994                 None                 None   \n",
       "Sales           9994   229.85800083049847     623.245100508681   \n",
       "Quantity        9994    3.789573744246548   2.2251096911414012   \n",
       "Discount        9994  0.15620272163297735  0.20645196782571612   \n",
       "Profit          9994     28.6568963077847    234.2601076909574   \n",
       "\n",
       "                                                               3  \\\n",
       "summary                                                      min   \n",
       "Row ID                                                         1   \n",
       "Order ID                                          CA-2015-100006   \n",
       "Ship Mode                                            First Class   \n",
       "Customer ID                                             AA-10315   \n",
       "Customer Name                                      Aaron Bergman   \n",
       "Segment                                                 Consumer   \n",
       "Country                                            United States   \n",
       "City                                                    Aberdeen   \n",
       "State                                                    Alabama   \n",
       "Postal Code                                               1040.0   \n",
       "Region                                                   Central   \n",
       "Product ID                                       FUR-BO-10000112   \n",
       "Category                                               Furniture   \n",
       "Sub-Category                                         Accessories   \n",
       "Product Name   \"While you Were Out\" Message Book, One Form pe...   \n",
       "Sales                                        0.44399999999999995   \n",
       "Quantity                                                       1   \n",
       "Discount                                                     0.0   \n",
       "Profit                                        -6599.978000000001   \n",
       "\n",
       "                                                4  \n",
       "summary                                       max  \n",
       "Row ID                                       9994  \n",
       "Order ID                           US-2018-169551  \n",
       "Ship Mode                          Standard Class  \n",
       "Customer ID                              ZD-21925  \n",
       "Customer Name                  Zuschuss Donatelli  \n",
       "Segment                               Home Office  \n",
       "Country                             United States  \n",
       "City                                         Yuma  \n",
       "State                                     Wyoming  \n",
       "Postal Code                                   NaN  \n",
       "Region                                       West  \n",
       "Product ID                        TEC-PH-10004977  \n",
       "Category                               Technology  \n",
       "Sub-Category                               Tables  \n",
       "Product Name   netTALK DUO VoIP Telephone Service  \n",
       "Sales                                    22638.48  \n",
       "Quantity                                       14  \n",
       "Discount                                      0.8  \n",
       "Profit                          8399.975999999999  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.describe() can be used to display the summary of the dataframe\n",
    "#df.toPandas() can be used to convert a spark dataframe into pandas dataframe\n",
    "\n",
    "superstore.describe().toPandas().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|     State|\n",
      "+----------+\n",
      "|   Alabama|\n",
      "|   Arizona|\n",
      "|  Arkansas|\n",
      "|California|\n",
      "|  Colorado|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df.distinct() gives the distinct values in a selected column\n",
    "\n",
    "superstore.select(\"State\").distinct().sort(\"State\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3002"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.sample() can be used to randomly select a sample of data from a dataframe\n",
    "\n",
    "seed = 5\n",
    "withReplacement = False\n",
    "fraction = 0.3\n",
    "\n",
    "superstore.sample(withReplacement, fraction, seed).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.randomSplit() can be used to split the data into two fractions\n",
    "\n",
    "df = superstore.randomSplit([0.25, 0.75], seed = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test:  2527 \n",
      " Train:  7467\n"
     ]
    }
   ],
   "source": [
    "print(\"Test: \", df[0].count(),\"\\n\",\"Train: \", df[1].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9994"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.union(df2) can be used to join two tables of same columns\n",
    "\n",
    "test = df[0]\n",
    "train = df[1]\n",
    "\n",
    "new = test.union(train)\n",
    "new.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc, asc, col\n",
    "\n",
    "newdf = superstore.select(\"State\").distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|    State|\n",
      "+---------+\n",
      "|     Utah|\n",
      "|Minnesota|\n",
      "|     Ohio|\n",
      "|   Oregon|\n",
      "| Arkansas|\n",
      "+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|     State|\n",
      "+----------+\n",
      "|   Alabama|\n",
      "|   Arizona|\n",
      "|  Arkansas|\n",
      "|California|\n",
      "|  Colorado|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf.sort(\"State\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|        State|\n",
      "+-------------+\n",
      "|      Wyoming|\n",
      "|    Wisconsin|\n",
      "|West Virginia|\n",
      "|   Washington|\n",
      "|     Virginia|\n",
      "+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df.orderBy(col(\"col name\")) can also be used to sort a dataframe column\n",
    "\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "newdf.orderBy(col(\"State\").desc()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "superstore.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "newdf = superstore.repartition(8)\n",
    "\n",
    "print(newdf.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "newdf = newdf.coalesce(2)\n",
    "\n",
    "print(newdf.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "superstore.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|       State|Quantity|\n",
      "+------------+--------+\n",
      "|  California|    7667|\n",
      "|    New York|    4224|\n",
      "|       Texas|    3724|\n",
      "|Pennsylvania|    2153|\n",
      "|    Illinois|    1845|\n",
      "+------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "superstore.where(col(\"State\") != \"Washington\")\\\n",
    ".select(\"State\", \"Quantity\")\\\n",
    ".groupBy(\"State\")\\\n",
    ".sum(\"Quantity\")\\\n",
    ".withColumnRenamed(\"sum(Quantity)\", \"Quantity\")\\\n",
    ".orderBy(col(\"Quantity\").desc())\\\n",
    ".show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+--------+\n",
      "|  State|     Ship Mode|Quantity|\n",
      "+-------+--------------+--------+\n",
      "|Alabama|   First Class|      33|\n",
      "|Alabama|      Same Day|       2|\n",
      "|Alabama|  Second Class|      77|\n",
      "|Alabama|Standard Class|     133|\n",
      "|Arizona|   First Class|     149|\n",
      "+-------+--------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "quantFilter = col(\"Quantity\") > 5\n",
    "catFilter = col(\"Category\") != \"Furniture\"\n",
    "cols = [\"State\", \"Ship Mode\", \"Quantity\"]\n",
    "superstore.where(quantFilter | catFilter)\\\n",
    ".select(\"State\", \"Ship Mode\", \"Quantity\")\\\n",
    ".groupBy(\"State\", \"Ship Mode\")\\\n",
    ".sum(\"Quantity\")\\\n",
    ".withColumnRenamed(\"sum(Quantity)\", \"Quantity\")\\\n",
    ".orderBy(cols, ascending = True)\\\n",
    ".show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|corr(Quantity, Profit)|\n",
      "+----------------------+\n",
      "|   0.06625318912428482|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import corr\n",
    "\n",
    "superstore.stat.corr(\"Quantity\", \"Profit\")\n",
    "superstore.select(corr(\"Quantity\", \"Profit\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------------------+------------------+-------------------+\n",
      "|summary|              Sales|            Profit|          Quantity|           Discount|\n",
      "+-------+-------------------+------------------+------------------+-------------------+\n",
      "|  count|               9994|              9994|              9994|               9994|\n",
      "|   mean| 229.85800083049847|  28.6568963077847| 3.789573744246548|0.15620272163297735|\n",
      "| stddev|   623.245100508681| 234.2601076909574|2.2251096911414012|0.20645196782571612|\n",
      "|    min|0.44399999999999995|-6599.978000000001|                 1|                0.0|\n",
      "|    max|           22638.48| 8399.975999999999|                14|                0.8|\n",
      "+-------+-------------------+------------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "superstore.select(\"Sales\", \"Profit\", \"Quantity\", \"Discount\").describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.24\n",
      "59.76\n",
      "206.112\n"
     ]
    }
   ],
   "source": [
    "colName = \"Sales\"\n",
    "quantileProbs = [0.25, 0.5, 0.75]\n",
    "relError = 0.05\n",
    "\n",
    "for i in superstore.stat.approxQuantile(colName, quantileProbs, relError):\n",
    "    print(round(i, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|ROW_ID|\n",
      "+------+\n",
      "|     0|\n",
      "|     1|\n",
      "|     2|\n",
      "|     3|\n",
      "|     4|\n",
      "+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "superstore.select(monotonically_increasing_id()).withColumnRenamed(\"monotonically_increasing_id()\", \"ROW_ID\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------------+---------------+\n",
      "|initcap(test row)|upper(test row)|lower(TEST ROW)|\n",
      "+-----------------+---------------+---------------+\n",
      "|         Test Row|       TEST ROW|       test row|\n",
      "|         Test Row|       TEST ROW|       test row|\n",
      "|         Test Row|       TEST ROW|       test row|\n",
      "|         Test Row|       TEST ROW|       test row|\n",
      "|         Test Row|       TEST ROW|       test row|\n",
      "+-----------------+---------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import initcap, lower, upper, lit\n",
    "\n",
    "superstore.select(initcap(lit(\"test row\")), upper(lit(\"test row\")), lower(lit(\"TEST ROW\"))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+-----+----------+----+\n",
      "|         ltrim|         rtrim| trim|      rpad|lpad|\n",
      "+--------------+--------------+-----+----------+----+\n",
      "|hello         |         hello|hello|hello     | hel|\n",
      "|hello         |         hello|hello|hello     | hel|\n",
      "|hello         |         hello|hello|hello     | hel|\n",
      "|hello         |         hello|hello|hello     | hel|\n",
      "|hello         |         hello|hello|hello     | hel|\n",
      "+--------------+--------------+-----+----------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import ltrim, rtrim, lpad, rpad, trim\n",
    "\n",
    "superstore.select(ltrim(lit(\"          hello         \")).alias(\"ltrim\"),\n",
    "                 rtrim(lit(\"         hello         \")).alias(\"rtrim\"),\n",
    "                 trim(lit(\"           hello        \")).alias(\"trim\"),\n",
    "                 rpad(lit(\"hello\"), 10, \" \").alias(\"rpad\"),\n",
    "                 lpad(lit(\"hello\"), 3, \" \").alias(\"lpad\")).show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|   latest orders|\n",
      "+----------------+\n",
      "|CA-latest-152156|\n",
      "|CA-latest-152156|\n",
      "|CA-latest-138688|\n",
      "|  US-2016-108966|\n",
      "|  US-2016-108966|\n",
      "+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "regex_string = \"2017|2018\"\n",
    "\n",
    "superstore.select(regexp_replace(col(\"Order ID\"), regex_string, \"latest\").alias(\"latest orders\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|translated state|\n",
      "+----------------+\n",
      "|        K6nt9cky|\n",
      "|        K6nt9cky|\n",
      "|      C5l7f8rn75|\n",
      "|         Fl8r7d5|\n",
      "|         Fl8r7d5|\n",
      "+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import translate\n",
    "\n",
    "superstore.select(translate(col(\"State\"), \"AEIOUaeiou\", \"0123456789\").alias(\"translated state\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------------------+\n",
      "|id |today     |now                   |\n",
      "+---+----------+----------------------+\n",
      "|0  |2019-01-17|2019-01-17 11:15:55.82|\n",
      "|1  |2019-01-17|2019-01-17 11:15:55.82|\n",
      "|2  |2019-01-17|2019-01-17 11:15:55.82|\n",
      "|3  |2019-01-17|2019-01-17 11:15:55.82|\n",
      "|4  |2019-01-17|2019-01-17 11:15:55.82|\n",
      "+---+----------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_date, current_timestamp\n",
    "\n",
    "dateDF = spark.range(10)\\\n",
    ".withColumn(\"today\", current_date())\\\n",
    ".withColumn(\"now\", current_timestamp())\n",
    "\n",
    "dateDF.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- today: date (nullable = false)\n",
      " |-- now: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dateDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'today', 'now']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dateDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dateDF.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+-----------------------+\n",
      "|date_add(today, 5)|date_sub(today, 5)|date_trunc(yyyy, today)|\n",
      "+------------------+------------------+-----------------------+\n",
      "|        2019-01-22|        2019-01-12|    2019-01-01 00:00:00|\n",
      "|        2019-01-22|        2019-01-12|    2019-01-01 00:00:00|\n",
      "|        2019-01-22|        2019-01-12|    2019-01-01 00:00:00|\n",
      "|        2019-01-22|        2019-01-12|    2019-01-01 00:00:00|\n",
      "|        2019-01-22|        2019-01-12|    2019-01-01 00:00:00|\n",
      "+------------------+------------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_add, date_sub, date_trunc\n",
    "\n",
    "dateDF.select(date_add(col(\"today\"), 5), \n",
    "              date_sub(col(\"today\"), 5), \n",
    "              date_trunc(\"yyyy\", col(\"today\"))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|to_date(`date`)|\n",
      "+---------------+\n",
      "|     2019-01-01|\n",
      "|     2019-01-01|\n",
      "|     2019-01-01|\n",
      "|     2019-01-01|\n",
      "|     2019-01-01|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "spark.range(5).withColumn(\"date\", lit(\"2019-01-01\")).select(to_date(col(\"date\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|datediff(week_ago, today)|\n",
      "+-------------------------+\n",
      "|                       -7|\n",
      "|                       -7|\n",
      "|                       -7|\n",
      "|                       -7|\n",
      "|                       -7|\n",
      "+-------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import datediff, months_between, to_date\n",
    "\n",
    "dateDF.withColumn(\"week_ago\", date_sub(col(\"today\"), 7)).select(datediff(col(\"week_ago\"), col(\"today\"))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|months_between(start, end)|\n",
      "+--------------------------+\n",
      "|               -1.64516129|\n",
      "|               -1.64516129|\n",
      "|               -1.64516129|\n",
      "|               -1.64516129|\n",
      "|               -1.64516129|\n",
      "+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateDF.select(to_date(lit(\"2016-04-01\")).alias(\"start\"),\n",
    "              to_date(lit(\"2016-05-21\")).alias(\"end\"))\\\n",
    "              .select(months_between(col(\"start\"), col(\"end\"))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|months_between(end, start)|\n",
      "+--------------------------+\n",
      "|              333.03225806|\n",
      "|              333.03225806|\n",
      "|              333.03225806|\n",
      "|              333.03225806|\n",
      "|              333.03225806|\n",
      "+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateDF.select(to_date(lit(\"01-04-1991\"), \"dd-MM-yyyy\").alias(\"start\"),\n",
    "             to_date(lit(\"02-01-2019\"), \"dd-MM-yyyy\").alias(\"end\"))\\\n",
    "            .select(months_between(col(\"end\"), col(\"start\"))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|coalesce(City, Product Name)|\n",
      "+----------------------------+\n",
      "|                   Henderson|\n",
      "|                   Henderson|\n",
      "|                 Los Angeles|\n",
      "|             Fort Lauderdale|\n",
      "|             Fort Lauderdale|\n",
      "+----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import coalesce\n",
    "\n",
    "superstore.select(coalesce(col(\"City\"), col(\"Product Name\"))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+-------------------+-------------------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+------------------+--------+--------+-------------------+\n",
      "|Row ID|      Order ID|         Order Date|          Ship Date|     Ship Mode|Customer ID|  Customer Name|  Segment|      Country|           City|     State|Postal Code|Region|     Product ID|       Category|Sub-Category|        Product Name|             Sales|Quantity|Discount|             Profit|\n",
      "+------+--------------+-------------------+-------------------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+------------------+--------+--------+-------------------+\n",
      "|     1|CA-2017-152156|2017-11-08 00:00:00|2017-11-11 00:00:00|  Second Class|   CG-12520|    Claire Gute| Consumer|United States|      Henderson|  Kentucky|    42420.0| South|FUR-BO-10001798|      Furniture|   Bookcases|Bush Somerset Col...|            261.96|       2|     0.0|            41.9136|\n",
      "|     2|CA-2017-152156|2017-11-08 00:00:00|2017-11-11 00:00:00|  Second Class|   CG-12520|    Claire Gute| Consumer|United States|      Henderson|  Kentucky|    42420.0| South|FUR-CH-10000454|      Furniture|      Chairs|Hon Deluxe Fabric...| 731.9399999999999|       3|     0.0| 219.58199999999997|\n",
      "|     3|CA-2017-138688|2017-06-12 00:00:00|2017-06-16 00:00:00|  Second Class|   DV-13045|Darrin Van Huff|Corporate|United States|    Los Angeles|California|    90036.0|  West|OFF-LA-10000240|Office Supplies|      Labels|Self-Adhesive Add...|             14.62|       2|     0.0| 6.8713999999999995|\n",
      "|     4|US-2016-108966|2016-10-11 00:00:00|2016-10-18 00:00:00|Standard Class|   SO-20335| Sean O'Donnell| Consumer|United States|Fort Lauderdale|   Florida|    33311.0| South|FUR-TA-10000577|      Furniture|      Tables|Bretford CR4500 S...|          957.5775|       5|    0.45|-383.03100000000006|\n",
      "|     5|US-2016-108966|2016-10-11 00:00:00|2016-10-18 00:00:00|Standard Class|   SO-20335| Sean O'Donnell| Consumer|United States|Fort Lauderdale|   Florida|    33311.0| South|OFF-ST-10000760|Office Supplies|     Storage|Eldon Fold 'N Rol...|22.368000000000002|       2|     0.2|  2.516399999999999|\n",
      "+------+--------------+-------------------+-------------------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+------------------+--------+--------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "superstore.na.drop().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+-------------------+-------------------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+------------------+--------+--------+-------------------+\n",
      "|Row ID|      Order ID|         Order Date|          Ship Date|     Ship Mode|Customer ID|  Customer Name|  Segment|      Country|           City|     State|Postal Code|Region|     Product ID|       Category|Sub-Category|        Product Name|             Sales|Quantity|Discount|             Profit|\n",
      "+------+--------------+-------------------+-------------------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+------------------+--------+--------+-------------------+\n",
      "|     1|CA-2017-152156|2017-11-08 00:00:00|2017-11-11 00:00:00|  Second Class|   CG-12520|    Claire Gute| Consumer|United States|      Henderson|  Kentucky|    42420.0| South|FUR-BO-10001798|      Furniture|   Bookcases|Bush Somerset Col...|            261.96|       2|     0.0|            41.9136|\n",
      "|     2|CA-2017-152156|2017-11-08 00:00:00|2017-11-11 00:00:00|  Second Class|   CG-12520|    Claire Gute| Consumer|United States|      Henderson|  Kentucky|    42420.0| South|FUR-CH-10000454|      Furniture|      Chairs|Hon Deluxe Fabric...| 731.9399999999999|       3|     0.0| 219.58199999999997|\n",
      "|     3|CA-2017-138688|2017-06-12 00:00:00|2017-06-16 00:00:00|  Second Class|   DV-13045|Darrin Van Huff|Corporate|United States|    Los Angeles|California|    90036.0|  West|OFF-LA-10000240|Office Supplies|      Labels|Self-Adhesive Add...|             14.62|       2|     0.0| 6.8713999999999995|\n",
      "|     4|US-2016-108966|2016-10-11 00:00:00|2016-10-18 00:00:00|Standard Class|   SO-20335| Sean O'Donnell| Consumer|United States|Fort Lauderdale|   Florida|    33311.0| South|FUR-TA-10000577|      Furniture|      Tables|Bretford CR4500 S...|          957.5775|       5|    0.45|-383.03100000000006|\n",
      "|     5|US-2016-108966|2016-10-11 00:00:00|2016-10-18 00:00:00|Standard Class|   SO-20335| Sean O'Donnell| Consumer|United States|Fort Lauderdale|   Florida|    33311.0| South|OFF-ST-10000760|Office Supplies|     Storage|Eldon Fold 'N Rol...|22.368000000000002|       2|     0.2|  2.516399999999999|\n",
      "+------+--------------+-------------------+-------------------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+------------------+--------+--------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "superstore.na.drop(\"all\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+-------------------+-------------------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+------------------+--------+--------+-------------------+\n",
      "|Row ID|      Order ID|         Order Date|          Ship Date|     Ship Mode|Customer ID|  Customer Name|  Segment|      Country|           City|     State|Postal Code|Region|     Product ID|       Category|Sub-Category|        Product Name|             Sales|Quantity|Discount|             Profit|\n",
      "+------+--------------+-------------------+-------------------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+------------------+--------+--------+-------------------+\n",
      "|     1|CA-2017-152156|2017-11-08 00:00:00|2017-11-11 00:00:00|  Second Class|   CG-12520|    Claire Gute| Consumer|United States|      Henderson|  Kentucky|    42420.0| South|FUR-BO-10001798|      Furniture|   Bookcases|Bush Somerset Col...|            261.96|       2|     0.0|            41.9136|\n",
      "|     2|CA-2017-152156|2017-11-08 00:00:00|2017-11-11 00:00:00|  Second Class|   CG-12520|    Claire Gute| Consumer|United States|      Henderson|  Kentucky|    42420.0| South|FUR-CH-10000454|      Furniture|      Chairs|Hon Deluxe Fabric...| 731.9399999999999|       3|     0.0| 219.58199999999997|\n",
      "|     3|CA-2017-138688|2017-06-12 00:00:00|2017-06-16 00:00:00|  Second Class|   DV-13045|Darrin Van Huff|Corporate|United States|    Los Angeles|California|    90036.0|  West|OFF-LA-10000240|Office Supplies|      Labels|Self-Adhesive Add...|             14.62|       2|     0.0| 6.8713999999999995|\n",
      "|     4|US-2016-108966|2016-10-11 00:00:00|2016-10-18 00:00:00|Standard Class|   SO-20335| Sean O'Donnell| Consumer|United States|Fort Lauderdale|   Florida|    33311.0| South|FUR-TA-10000577|      Furniture|      Tables|Bretford CR4500 S...|          957.5775|       5|    0.45|-383.03100000000006|\n",
      "|     5|US-2016-108966|2016-10-11 00:00:00|2016-10-18 00:00:00|Standard Class|   SO-20335| Sean O'Donnell| Consumer|United States|Fort Lauderdale|   Florida|    33311.0| South|OFF-ST-10000760|Office Supplies|     Storage|Eldon Fold 'N Rol...|22.368000000000002|       2|     0.2|  2.516399999999999|\n",
      "+------+--------------+-------------------+-------------------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+------------------+--------+--------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "superstore.na.drop(\"all\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+-------------------+-------------------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+------------------+--------+--------+-------------------+\n",
      "|Row ID|      Order ID|         Order Date|          Ship Date|     Ship Mode|Customer ID|  Customer Name|  Segment|      Country|           City|     State|Postal Code|Region|     Product ID|       Category|Sub-Category|        Product Name|             Sales|Quantity|Discount|             Profit|\n",
      "+------+--------------+-------------------+-------------------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+------------------+--------+--------+-------------------+\n",
      "|     1|CA-2017-152156|2017-11-08 00:00:00|2017-11-11 00:00:00|  Second Class|   CG-12520|    Claire Gute| Consumer|United States|      Henderson|  Kentucky|    42420.0| South|FUR-BO-10001798|      Furniture|   Bookcases|Bush Somerset Col...|            261.96|       2|     0.0|            41.9136|\n",
      "|     2|CA-2017-152156|2017-11-08 00:00:00|2017-11-11 00:00:00|  Second Class|   CG-12520|    Claire Gute| Consumer|United States|      Henderson|  Kentucky|    42420.0| South|FUR-CH-10000454|      Furniture|      Chairs|Hon Deluxe Fabric...| 731.9399999999999|       3|     0.0| 219.58199999999997|\n",
      "|     3|CA-2017-138688|2017-06-12 00:00:00|2017-06-16 00:00:00|  Second Class|   DV-13045|Darrin Van Huff|Corporate|United States|    Los Angeles|California|    90036.0|  West|OFF-LA-10000240|Office Supplies|      Labels|Self-Adhesive Add...|             14.62|       2|     0.0| 6.8713999999999995|\n",
      "|     4|US-2016-108966|2016-10-11 00:00:00|2016-10-18 00:00:00|Standard Class|   SO-20335| Sean O'Donnell| Consumer|United States|Fort Lauderdale|   Florida|    33311.0| South|FUR-TA-10000577|      Furniture|      Tables|Bretford CR4500 S...|          957.5775|       5|    0.45|-383.03100000000006|\n",
      "|     5|US-2016-108966|2016-10-11 00:00:00|2016-10-18 00:00:00|Standard Class|   SO-20335| Sean O'Donnell| Consumer|United States|Fort Lauderdale|   Florida|    33311.0| South|OFF-ST-10000760|Office Supplies|     Storage|Eldon Fold 'N Rol...|22.368000000000002|       2|     0.2|  2.516399999999999|\n",
      "+------+--------------+-------------------+-------------------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+------------------+--------+--------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "superstore.na.drop(\"all\", subset = [\"Order Id\", \"Order Date\"]).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+-------------------+-------------------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+------------------+--------+--------+-------------------+\n",
      "|Row ID|      Order ID|         Order Date|          Ship Date|     Ship Mode|Customer ID|  Customer Name|  Segment|      Country|           City|     State|Postal Code|Region|     Product ID|       Category|Sub-Category|        Product Name|             Sales|Quantity|Discount|             Profit|\n",
      "+------+--------------+-------------------+-------------------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+------------------+--------+--------+-------------------+\n",
      "|     1|CA-2017-152156|2017-11-08 00:00:00|2017-11-11 00:00:00|  Second Class|   CG-12520|    Claire Gute| Consumer|United States|      Henderson|  Kentucky|    42420.0| South|FUR-BO-10001798|      Furniture|   Bookcases|Bush Somerset Col...|            261.96|       2|     0.0|            41.9136|\n",
      "|     2|CA-2017-152156|2017-11-08 00:00:00|2017-11-11 00:00:00|  Second Class|   CG-12520|    Claire Gute| Consumer|United States|      Henderson|  Kentucky|    42420.0| South|FUR-CH-10000454|      Furniture|      Chairs|Hon Deluxe Fabric...| 731.9399999999999|       3|     0.0| 219.58199999999997|\n",
      "|     3|CA-2017-138688|2017-06-12 00:00:00|2017-06-16 00:00:00|  Second Class|   DV-13045|Darrin Van Huff|Corporate|United States|    Los Angeles|California|    90036.0|  West|OFF-LA-10000240|Office Supplies|      Labels|Self-Adhesive Add...|             14.62|       2|     0.0| 6.8713999999999995|\n",
      "|     4|US-2016-108966|2016-10-11 00:00:00|2016-10-18 00:00:00|Standard Class|   SO-20335| Sean O'Donnell| Consumer|United States|Fort Lauderdale|   Florida|    33311.0| South|FUR-TA-10000577|      Furniture|      Tables|Bretford CR4500 S...|          957.5775|       5|    0.45|-383.03100000000006|\n",
      "|     5|US-2016-108966|2016-10-11 00:00:00|2016-10-18 00:00:00|Standard Class|   SO-20335| Sean O'Donnell| Consumer|United States|Fort Lauderdale|   Florida|    33311.0| South|OFF-ST-10000760|Office Supplies|     Storage|Eldon Fold 'N Rol...|22.368000000000002|       2|     0.2|  2.516399999999999|\n",
      "+------+--------------+-------------------+-------------------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+------------------+--------+--------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "superstore.na.fill(\"All null values become this string\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+-------------------+-------------------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+------------------+--------+--------+-------------------+\n",
      "|Row ID|      Order ID|         Order Date|          Ship Date|     Ship Mode|Customer ID|  Customer Name|  Segment|      Country|           City|     State|Postal Code|Region|     Product ID|       Category|Sub-Category|        Product Name|             Sales|Quantity|Discount|             Profit|\n",
      "+------+--------------+-------------------+-------------------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+------------------+--------+--------+-------------------+\n",
      "|     1|CA-2017-152156|2017-11-08 00:00:00|2017-11-11 00:00:00|  Second Class|   CG-12520|    Claire Gute| Consumer|United States|      Henderson|  Kentucky|    42420.0| South|FUR-BO-10001798|      Furniture|   Bookcases|Bush Somerset Col...|            261.96|       2|     0.0|            41.9136|\n",
      "|     2|CA-2017-152156|2017-11-08 00:00:00|2017-11-11 00:00:00|  Second Class|   CG-12520|    Claire Gute| Consumer|United States|      Henderson|  Kentucky|    42420.0| South|FUR-CH-10000454|      Furniture|      Chairs|Hon Deluxe Fabric...| 731.9399999999999|       3|     0.0| 219.58199999999997|\n",
      "|     3|CA-2017-138688|2017-06-12 00:00:00|2017-06-16 00:00:00|  Second Class|   DV-13045|Darrin Van Huff|Corporate|United States|    Los Angeles|California|    90036.0|  West|OFF-LA-10000240|Office Supplies|      Labels|Self-Adhesive Add...|             14.62|       2|     0.0| 6.8713999999999995|\n",
      "|     4|US-2016-108966|2016-10-11 00:00:00|2016-10-18 00:00:00|Standard Class|   SO-20335| Sean O'Donnell| Consumer|United States|Fort Lauderdale|   Florida|    33311.0| South|FUR-TA-10000577|      Furniture|      Tables|Bretford CR4500 S...|          957.5775|       5|    0.45|-383.03100000000006|\n",
      "|     5|US-2016-108966|2016-10-11 00:00:00|2016-10-18 00:00:00|Standard Class|   SO-20335| Sean O'Donnell| Consumer|United States|Fort Lauderdale|   Florida|    33311.0| South|OFF-ST-10000760|Office Supplies|     Storage|Eldon Fold 'N Rol...|22.368000000000002|       2|     0.2|  2.516399999999999|\n",
      "+------+--------------+-------------------+-------------------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+------------------+--------+--------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "superstore.na.replace([\" \"], [\"UNKNOWN\"], \"Description\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#struct funtion is used to create a complex column by combining multiple columns so that they can be later queried\n",
    "\n",
    "from pyspark.sql.functions import struct\n",
    "\n",
    "df_test = superstore.select(struct(\"Row ID\", \"Order ID\").alias(\"complex\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|      Order ID|\n",
      "+--------------+\n",
      "|CA-2017-152156|\n",
      "|CA-2017-152156|\n",
      "|CA-2017-138688|\n",
      "|US-2016-108966|\n",
      "|US-2016-108966|\n",
      "+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.select(\"complex.Order ID\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|First_and_Last_Names[1]|\n",
      "+-----------------------+\n",
      "|                   Gute|\n",
      "|                   Gute|\n",
      "|                    Van|\n",
      "|              O'Donnell|\n",
      "|              O'Donnell|\n",
      "+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#split function is used to split rows of a column into arrays\n",
    "\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "superstore.select(split(col(\"Customer Name\"), \" \").alias(\"First_and_Last_Names\"))\\\n",
    ".selectExpr(\"First_and_Last_Names[1]\")\\\n",
    ".show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|name_split|\n",
      "+----------+\n",
      "|         2|\n",
      "|         2|\n",
      "|         3|\n",
      "|         2|\n",
      "|         2|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#size function can be used to find the size of the array\n",
    "\n",
    "from pyspark.sql.functions import size, array_contains\n",
    "\n",
    "superstore.select(size(split(col(\"Customer Name\"), \" \")).alias(\"name_split\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|is_hoffman|\n",
      "+----------+\n",
      "|     false|\n",
      "|     false|\n",
      "|     false|\n",
      "|     false|\n",
      "|     false|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#array_contains can be used to check whether the array contains a given value\n",
    "\n",
    "superstore.select(array_contains(split(col(\"Customer Name\"), \" \"), \"Hoffman\").alias(\"is_hoffman\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+--------+\n",
      "|  Customer Name|           splitted|exploded|\n",
      "+---------------+-------------------+--------+\n",
      "|    Claire Gute|     [Claire, Gute]|  Claire|\n",
      "|    Claire Gute|     [Claire, Gute]|    Gute|\n",
      "|    Claire Gute|     [Claire, Gute]|  Claire|\n",
      "|    Claire Gute|     [Claire, Gute]|    Gute|\n",
      "|Darrin Van Huff|[Darrin, Van, Huff]|  Darrin|\n",
      "+---------------+-------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#explode function can be used to create new rows from the indicidual values of an array\n",
    "\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "superstore.withColumn(\"splitted\", split(col(\"Customer Name\"), \" \"))\\\n",
    ".withColumn(\"exploded\", explode(col(\"splitted\")))\\\n",
    ".select(\"Customer Name\",\"splitted\", \"exploded\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n",
      "|mapped                             |\n",
      "+-----------------------------------+\n",
      "|[Claire Gute -> CA-2017-152156]    |\n",
      "|[Claire Gute -> CA-2017-152156]    |\n",
      "|[Darrin Van Huff -> CA-2017-138688]|\n",
      "|[Sean O'Donnell -> US-2016-108966] |\n",
      "|[Sean O'Donnell -> US-2016-108966] |\n",
      "+-----------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#map function can be used to create key value pairs of columns\n",
    "\n",
    "from pyspark.sql.functions import create_map\n",
    "\n",
    "superstore.select(create_map(col(\"Customer Name\"), col(\"Order ID\")).alias(\"mapped\")).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|mapped[Claire Gute]|\n",
      "+-------------------+\n",
      "|     CA-2017-152156|\n",
      "|     CA-2017-152156|\n",
      "|               null|\n",
      "|               null|\n",
      "|               null|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#maps can be queried\n",
    "\n",
    "superstore.select(create_map(col(\"Customer Name\"), col(\"Order ID\")).alias(\"mapped\"))\\\n",
    ".selectExpr(\"mapped['Claire Gute']\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#handling jason data\n",
    "\n",
    "jsondf = spark.range(1).selectExpr(\"\"\" '{\"myJsonKey\": \n",
    "                                                {\"myJsonValues\": [1, 2, 3]}}' \n",
    "                                                    as jsonString \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------------+\n",
      "|column|jsonKey                 |\n",
      "+------+------------------------+\n",
      "|1     |{\"myJsonValues\":[1,2,3]}|\n",
      "+------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import get_json_object, json_tuple\n",
    "\n",
    "jsondf.select(\n",
    "    get_json_object(col(\"jsonString\"), \"$.myJsonKey.myJsonValues[0]\").alias(\"column\"), \n",
    "    json_tuple(col(\"jsonString\"), \"myJsonKey\").alias(\"jsonKey\")).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------+\n",
      "|structstojson(myStruct)                                        |\n",
      "+---------------------------------------------------------------+\n",
      "|{\"Order ID\":\"CA-2017-152156\",\"Customer Name\":\"Claire Gute\"}    |\n",
      "|{\"Order ID\":\"CA-2017-152156\",\"Customer Name\":\"Claire Gute\"}    |\n",
      "|{\"Order ID\":\"CA-2017-138688\",\"Customer Name\":\"Darrin Van Huff\"}|\n",
      "|{\"Order ID\":\"US-2016-108966\",\"Customer Name\":\"Sean O'Donnell\"} |\n",
      "|{\"Order ID\":\"US-2016-108966\",\"Customer Name\":\"Sean O'Donnell\"} |\n",
      "+---------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_json\n",
    "\n",
    "superstore.selectExpr(\"(`Order ID`, `Customer Name`) as myStruct\")\\\n",
    ".select(to_json(col(\"myStruct\"))).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
