{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_excel(\"superstore.xls\")\n",
    "superstore = spark.createDataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.printSchema() shows the schema\n",
    "\n",
    "superstore.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.selectExpr(\"`col name` as id\") gives a df with col name renamed in display\n",
    "\n",
    "superstore.selectExpr(\"`Order ID` as id\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.where(\"filter condition\") is used to filter the data based on a given logical condition\n",
    "\n",
    "superstore.where(\"Quantity > 10\")\\\n",
    "          .selectExpr(\"`Order ID`\", \"`Postal Code`\", \"`Quantity`\")\\\n",
    "          .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import column\n",
    "\n",
    "#df.sort(col(\"col name\")).desc() orders the dataframe in descending order\n",
    "\n",
    "superstore.where(column(\"Category\") == \"Furniture\")\\\n",
    "          .select(\"Order ID\", \"Postal Code\", \"Quantity\")\\\n",
    "          .sort(column(\"Quantity\").desc())\\\n",
    "          .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multiple df.where fucntions can be used to filter more conditions\n",
    "\n",
    "superstore.where(column(\"Category\") != \"Furniture\")\\\n",
    "          .where(column(\"Region\") == \"West\")\\\n",
    "          .selectExpr(\"Category\", \"Region\", \"Quantity\")\\\n",
    "          .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.createOrReplaceTempView(\"view name\") can be used to create temp view which can be used to query tables using spark.sql\n",
    "\n",
    "superstore.createOrReplaceTempView(\"superstore_view\")\n",
    "\n",
    "spark.sql(\"\"\"DESCRIBE FORMATTED superstore_view\"\"\").show(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"SQL QUERY\") can be used to query temp views\n",
    "\n",
    "spark.sql(\"SELECT Category, Region, Quantity FROM superstore_view\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"SELECT  Category, Region, SUM(Quantity)\n",
    "FROM superstore_view \n",
    "GROUP BY Category, Region\n",
    "ORDER BY SUM(Quantity) DESC LIMIT 10\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\" SELECT Category, (Region = \"Furniture\" AND Quantity > 10 OR SALES > 1000) as test\n",
    "FROM superstore_view WHERE Region = \"Furniture\" AND Quantity > 10 OR SALES > 1000\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.describe() can be used to display the summary of the dataframe\n",
    "#df.toPandas() can be used to convert a spark dataframe into pandas dataframe\n",
    "\n",
    "superstore.describe().toPandas().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.distinct() gives the distinct values in a selected column\n",
    "\n",
    "superstore.select(\"State\").distinct().sort(\"State\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.sample() can be used to randomly select a sample of data from a dataframe\n",
    "\n",
    "seed = 5\n",
    "withReplacement = False\n",
    "fraction = 0.3\n",
    "\n",
    "superstore.sample(withReplacement, fraction, seed).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.randomSplit() can be used to split the data into two fractions\n",
    "\n",
    "df = superstore.randomSplit([0.25, 0.75], seed = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test: \", df[0].count(),\"\\n\",\"Train: \", df[1].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.union(df2) can be used to join two tables of same columns\n",
    "\n",
    "test = df[0]\n",
    "train = df[1]\n",
    "\n",
    "new = test.union(train)\n",
    "new.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc, asc, col\n",
    "\n",
    "newdf = superstore.select(\"State\").distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.sort(\"State\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.orderBy(col(\"col name\")) can also be used to sort a dataframe column\n",
    "\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "newdf.orderBy(col(\"State\").desc()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "superstore.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = superstore.repartition(8)\n",
    "\n",
    "print(newdf.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = newdf.coalesce(2)\n",
    "\n",
    "print(newdf.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "superstore.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "superstore.where(col(\"State\") != \"Washington\")\\\n",
    ".select(\"State\", \"Quantity\")\\\n",
    ".groupBy(\"State\")\\\n",
    ".sum(\"Quantity\")\\\n",
    ".withColumnRenamed(\"sum(Quantity)\", \"Quantity\")\\\n",
    ".orderBy(col(\"Quantity\").desc())\\\n",
    ".show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantFilter = col(\"Quantity\") > 5\n",
    "catFilter = col(\"Category\") != \"Furniture\"\n",
    "cols = [\"State\", \"Ship Mode\", \"Quantity\"]\n",
    "superstore.where(quantFilter | catFilter)\\\n",
    ".select(\"State\", \"Ship Mode\", \"Quantity\")\\\n",
    ".groupBy(\"State\", \"Ship Mode\")\\\n",
    ".sum(\"Quantity\")\\\n",
    ".withColumnRenamed(\"sum(Quantity)\", \"Quantity\")\\\n",
    ".orderBy(cols, ascending = True)\\\n",
    ".show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import corr\n",
    "\n",
    "superstore.stat.corr(\"Quantity\", \"Profit\")\n",
    "superstore.select(corr(\"Quantity\", \"Profit\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "superstore.select(\"Sales\", \"Profit\", \"Quantity\", \"Discount\").describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colName = \"Sales\"\n",
    "quantileProbs = [0.25, 0.5, 0.75]\n",
    "relError = 0.05\n",
    "\n",
    "for i in superstore.stat.approxQuantile(colName, quantileProbs, relError):\n",
    "    print(round(i, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "superstore.select(monotonically_increasing_id()).withColumnRenamed(\"monotonically_increasing_id()\", \"ROW_ID\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import initcap, lower, upper, lit\n",
    "\n",
    "superstore.select(initcap(lit(\"test row\")), upper(lit(\"test row\")), lower(lit(\"TEST ROW\"))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import ltrim, rtrim, lpad, rpad, trim\n",
    "\n",
    "superstore.select(ltrim(lit(\"          hello         \")).alias(\"ltrim\"),\n",
    "                 rtrim(lit(\"         hello         \")).alias(\"rtrim\"),\n",
    "                 trim(lit(\"           hello        \")).alias(\"trim\"),\n",
    "                 rpad(lit(\"hello\"), 10, \" \").alias(\"rpad\"),\n",
    "                 lpad(lit(\"hello\"), 3, \" \").alias(\"lpad\")).show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "regex_string = \"2017|2018\"\n",
    "\n",
    "superstore.select(regexp_replace(col(\"Order ID\"), regex_string, \"latest\").alias(\"latest orders\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import translate\n",
    "\n",
    "superstore.select(translate(col(\"State\"), \"AEIOUaeiou\", \"0123456789\").alias(\"translated state\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date, current_timestamp\n",
    "\n",
    "dateDF = spark.range(10)\\\n",
    ".withColumn(\"today\", current_date())\\\n",
    ".withColumn(\"now\", current_timestamp())\n",
    "\n",
    "dateDF.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dateDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateDF.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_add, date_sub, date_trunc\n",
    "\n",
    "dateDF.select(date_add(col(\"today\"), 5), \n",
    "              date_sub(col(\"today\"), 5), \n",
    "              date_trunc(\"yyyy\", col(\"today\"))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "spark.range(5).withColumn(\"date\", lit(\"2019-01-01\")).select(to_date(col(\"date\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import datediff, months_between, to_date\n",
    "\n",
    "dateDF.withColumn(\"week_ago\", date_sub(col(\"today\"), 7)).select(datediff(col(\"week_ago\"), col(\"today\"))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateDF.select(to_date(lit(\"2016-04-01\")).alias(\"start\"),\n",
    "              to_date(lit(\"2016-05-21\")).alias(\"end\"))\\\n",
    "              .select(months_between(col(\"start\"), col(\"end\"))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateDF.select(to_date(lit(\"01-04-1991\"), \"dd-MM-yyyy\").alias(\"start\"),\n",
    "             to_date(lit(\"02-01-2019\"), \"dd-MM-yyyy\").alias(\"end\"))\\\n",
    "            .select(months_between(col(\"end\"), col(\"start\"))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import coalesce\n",
    "\n",
    "superstore.select(coalesce(col(\"City\"), col(\"Product Name\"))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "superstore.na.drop().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "superstore.na.drop(\"all\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "superstore.na.drop(\"all\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "superstore.na.drop(\"all\", subset = [\"Order Id\", \"Order Date\"]).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "superstore.na.fill(\"All null values become this string\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "superstore.na.replace([\" \"], [\"UNKNOWN\"], \"Description\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#struct funtion is used to create a complex column by combining multiple columns so that they can be later queried\n",
    "\n",
    "from pyspark.sql.functions import struct\n",
    "\n",
    "df_test = superstore.select(struct(\"Row ID\", \"Order ID\").alias(\"complex\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.select(\"complex.Order ID\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split function is used to split rows of a column into arrays\n",
    "\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "superstore.select(split(col(\"Customer Name\"), \" \").alias(\"First_and_Last_Names\"))\\\n",
    ".selectExpr(\"First_and_Last_Names[1]\")\\\n",
    ".show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#size function can be used to find the size of the array\n",
    "\n",
    "from pyspark.sql.functions import size, array_contains\n",
    "\n",
    "superstore.select(size(split(col(\"Customer Name\"), \" \")).alias(\"name_split\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#array_contains can be used to check whether the array contains a given value\n",
    "\n",
    "superstore.select(array_contains(split(col(\"Customer Name\"), \" \"), \"Hoffman\").alias(\"is_hoffman\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explode function can be used to create new rows from the indicidual values of an array\n",
    "\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "superstore.withColumn(\"splitted\", split(col(\"Customer Name\"), \" \"))\\\n",
    ".withColumn(\"exploded\", explode(col(\"splitted\")))\\\n",
    ".select(\"Customer Name\",\"splitted\", \"exploded\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map function can be used to create key value pairs of columns\n",
    "\n",
    "from pyspark.sql.functions import create_map\n",
    "\n",
    "superstore.select(create_map(col(\"Customer Name\"), col(\"Order ID\")).alias(\"mapped\")).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#maps can be queried\n",
    "\n",
    "superstore.select(create_map(col(\"Customer Name\"), col(\"Order ID\")).alias(\"mapped\"))\\\n",
    ".selectExpr(\"mapped['Claire Gute']\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#handling jason data\n",
    "\n",
    "jsondf = spark.range(1).selectExpr(\"\"\" '{\"myJsonKey\": \n",
    "                                                {\"myJsonValues\": [1, 2, 3]}}' \n",
    "                                                    as jsonString \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import get_json_object, json_tuple\n",
    "\n",
    "jsondf.select(\n",
    "    get_json_object(col(\"jsonString\"), \"$.myJsonKey.myJsonValues[0]\").alias(\"column\"), \n",
    "    json_tuple(col(\"jsonString\"), \"myJsonKey\").alias(\"jsonKey\")).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_json\n",
    "\n",
    "superstore.selectExpr(\"(`Order ID`, `Customer Name`) as myStruct\")\\\n",
    ".select(to_json(col(\"myStruct\"))).show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count of the Order IDs\n",
    "\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "superstore.select(count(\"Order ID\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count of distinct Order ID\n",
    "\n",
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "superstore.select(countDistinct(\"Order ID\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find only a certain degree of of count distinct\n",
    "\n",
    "from pyspark.sql.functions import approx_count_distinct\n",
    "\n",
    "superstore.select(approx_count_distinct(\"Order ID\", 0.01)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the first and the last items in a columns\n",
    "\n",
    "from pyspark.sql.functions import first, last\n",
    "\n",
    "superstore.select(first(\"Order ID\"), last(\"Order ID\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the min and max for a column\n",
    "\n",
    "from pyspark.sql.functions import min, max\n",
    "\n",
    "superstore.select(min(\"Quantity\"), max(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the sum of a numerical column\n",
    "\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "superstore.select(sum(\"Sales\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the disticnt sum of a column\n",
    "\n",
    "from pyspark.sql.functions import sumDistinct\n",
    "\n",
    "superstore.select(sumDistinct(\"Sales\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the sum, max, min, expr of a column\n",
    "\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "superstore.select(sum(\"Quantity\").alias(\"total_products_sold\"), \n",
    "                  countDistinct(\"Customer ID\").alias(\"distinct_products_sold\"),\n",
    "                 sum(\"Sales\").alias(\"total_sales_amount\"),\n",
    "                 avg(\"Sales\").alias(\"mean_sales\")).selectExpr(\"total_products_sold\", \n",
    "                                                             \"distinct_products_sold\",\n",
    "                                                             \"total_sales_amount\",\n",
    "                                                             \"mean_sales\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import var_pop, stddev_pop, var_samp, stddev_samp\n",
    "\n",
    "superstore.select(var_pop(\"Quantity\"), var_samp(\"Quantity\"), \n",
    "                  stddev_pop(\"Quantity\"), stddev_samp(\"Quantity\")).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import skewness, kurtosis\n",
    "\n",
    "superstore.select(skewness(\"Sales\"), kurtosis(\"Sales\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aggregarte complex types e.g aggregate the number of states\n",
    "\n",
    "from pyspark.sql.functions import collect_list, collect_set\n",
    "\n",
    "superstore.agg(collect_list(\"State\"), collect_set(\"State\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#groupby condition on dataframes\n",
    "\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "superstore.groupby(\"Region\", \"State\").agg(expr(\"count('Order ID')\").alias(\"Order Count\"))\\\n",
    ".orderBy(\"Region\", \"State\").show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group by functions \n",
    "\n",
    "superstore.groupby(\"Order ID\").agg(expr(\"max(Quantity)\").alias(\"max_quantity\"), stddev_pop(\"Quantity\"))\\\n",
    ".orderBy(\"max_quantity\", ascending = False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#window functions ***IMPORTANT***\n",
    "\n",
    "dfWithDate = superstore.withColumn(\"date\", to_date(col(\"Order Date\"), \"MM/d/yyyy H:mm\"))\n",
    "dfWithDate = dfWithDate.select(col(\"Category\"), col(\"date\"), col(\"Sales\"))\n",
    "\n",
    "dfWithDate.createOrReplaceTempView(\"dfWithDate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfWithDate.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "windowSpec = Window\\\n",
    ".partitionBy(\"date\", \"Category\")\\\n",
    ".orderBy(desc(\"Sales\"))\\\n",
    ".rowsBetween(Window.unboundedPreceding, Window.currentRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxPurchaseQuantity = max(col(\"Sales\")).over(windowSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import dense_rank, rank, row_number\n",
    "\n",
    "purchaseDenseRank = dense_rank().over(windowSpec)\n",
    "purchaseRank = rank().over(windowSpec)\n",
    "rownum = row_number().over(windowSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfWithDate.where(\"`Category` IS NOT NULL\").orderBy(\"date\")\\\n",
    ".select(col(\"Category\"), \n",
    "        col(\"date\"), \n",
    "        col(\"Sales\"), \n",
    "        purchaseRank.alias(\"rank\"), \n",
    "        purchaseDenseRank.alias(\"dense_rank\"), \n",
    "        maxPurchaseQuantity.alias(\"max_sales\"),\n",
    "       rownum.alias(\"row id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#roll up\n",
    "\n",
    "dfNoNull = dfWithDate.drop()\n",
    "dfNoNull.createOrReplaceTempView(\"dfNoNull\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNoNull.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grouping sets can be used to group data across multiple groups. Only available in SQL\n",
    "\n",
    "spark.sql(\"SELECT Category, date, sum(Sales) FROM dfNoNull GROUP BY Category, date GROUPING SETS ((Category, date), ()) ORDER BY Category ASC, date DESC\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#roll up function\n",
    "\n",
    "rolledUpDF = dfNoNull.rollup(\"date\", \"Category\")\\\n",
    ".agg(sum(\"Sales\")).selectExpr(\"date\", \"Category\", \"`sum(Sales)` as total_sales\")\\\n",
    ".orderBy(\"date\")\n",
    "\n",
    "rolledUpDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolledUpDF.where(\"Category IS NULL\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolledUpDF.where(\"date IS NULL\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cube function\n",
    "\n",
    "cubeDF = dfNoNull.cube(\"date\", \"Category\", \"Sales\")\\\n",
    ".agg(max(\"Sales\"))\\\n",
    ".selectExpr(\"date\", \"Category\", \"`max(Sales)` as highest_sales\")\\\n",
    ".orderBy(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cubeDF.sort(\"date\", ascending = False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cubeDF.where(\"Category IS NULL\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivotDF = dfWithDate.groupby(\"date\").pivot(\"Category\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivotDF.where(\"`date` > '2018-11-30'\").orderBy(\"date\", ascending = True).show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id, col, year\n",
    "\n",
    "df1 = superstore.select(monotonically_increasing_id(), year(\"Order Date\"))\\\n",
    ".withColumnRenamed(\"year(Order Date)\", \"date1\")\\\n",
    ".withColumnRenamed(\"monotonically_increasing_id()\", \"ROW_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = superstore.select(monotonically_increasing_id(), year(\"Ship Date\"))\\\n",
    ".withColumnRenamed(\"year(Ship Date)\", \"date2\")\\\n",
    ".withColumnRenamed(\"monotonically_increasing_id()\", \"ROW_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.join(df1, df1.ROW_ID == df2.ROW_ID)\\\n",
    ".withColumn(\"difference\", df2.date2 - df1.date1)\\\n",
    ".selectExpr(\"date1\", \"date2\", \"difference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.where(\"difference > 0\").orderBy(\"date1\", ascending = False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.describe().toPandas().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pair RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pair RDD with tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "person = spark.createDataFrame([\n",
    "    (0, \"Bill Chambers\", 0, [100]),\n",
    "    (1, \"Matei Zaharia\", 1, [500, 250, 100]),\n",
    "    (2, \"Michael Armbrust\", 1, [250, 100])]).toDF(\"id\", \"name\", \"graduate_course\", \"spark_status\")\n",
    "\n",
    "graduateProgram = spark.createDataFrame([\n",
    "    (0, \"Masters\", \"School of Information\", \"UC Berkeley\"),\n",
    "    (2, \"Masters\", \"EECS\", \"UC Berkeley\"),\n",
    "    (1, \"Ph.D.\", \"EECS\", \"UC Berkeley\")]).toDF(\"id\", \"degree\", \"department\", \"school\")\n",
    "\n",
    "sparkStatus = spark.createDataFrame([\n",
    "    (500, \"Vice President\"),\n",
    "    (250, \"PMC Member\"),\n",
    "    (100, \"Contributor\")]).toDF(\"id\", \"status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "person.createOrReplaceTempView(\"person\")\n",
    "graduateProgram.createOrReplaceTempView(\"graduateProgram\")\n",
    "sparkStatus.createOrReplaceTempView(\"sparkStatus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inner Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+---------------+---------------+---+-------+--------------------+-----------+\n",
      "| id|            name|graduate_course|   spark_status| id| degree|          department|     school|\n",
      "+---+----------------+---------------+---------------+---+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|              0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|   Matei Zaharia|              1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  2|Michael Armbrust|              1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+----------------+---------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinCondition = person[\"graduate_course\"] == graduateProgram[\"id\"]\n",
    "\n",
    "joinedDf = person.join(graduateProgram, joinCondition)\n",
    "joinedDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+---------------+---------------+---+-------+--------------------+-----------+\n",
      "| id|            name|graduate_course|   spark_status| id| degree|          department|     school|\n",
      "+---+----------------+---------------+---------------+---+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|              0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|   Matei Zaharia|              1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  2|Michael Armbrust|              1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+----------------+---------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM person INNER JOIN graduateProgram ON person.graduate_course = graduateProgram.id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outer Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+---------------+---------------+---+-------+--------------------+-----------+\n",
      "|  id|            name|graduate_course|   spark_status| id| degree|          department|     school|\n",
      "+----+----------------+---------------+---------------+---+-------+--------------------+-----------+\n",
      "|   0|   Bill Chambers|              0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|   1|   Matei Zaharia|              1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|   2|Michael Armbrust|              1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|null|            null|           null|           null|  2|Masters|                EECS|UC Berkeley|\n",
      "+----+----------------+---------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinType = \"outer\"\n",
    "\n",
    "joinedDf = person.join(graduateProgram, joinCondition, joinType)\n",
    "joinedDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+---------------+---------------+---+-------+--------------------+-----------+\n",
      "|  id|            name|graduate_course|   spark_status| id| degree|          department|     school|\n",
      "+----+----------------+---------------+---------------+---+-------+--------------------+-----------+\n",
      "|   0|   Bill Chambers|              0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|   1|   Matei Zaharia|              1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|   2|Michael Armbrust|              1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|null|            null|           null|           null|  2|Masters|                EECS|UC Berkeley|\n",
      "+----+----------------+---------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM person FULL OUTER JOIN graduateProgram ON person.graduate_course = graduateProgram.id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left Outer Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+---------------+---------------+---+-------+--------------------+-----------+\n",
      "| id|            name|graduate_course|   spark_status| id| degree|          department|     school|\n",
      "+---+----------------+---------------+---------------+---+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|              0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|   Matei Zaharia|              1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  2|Michael Armbrust|              1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+----------------+---------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinType = \"left_outer\"\n",
    "\n",
    "joinedDf = person.join(graduateProgram, joinCondition, joinType)\n",
    "joinedDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+---------------+---------------+---+-------+--------------------+-----------+\n",
      "| id|            name|graduate_course|   spark_status| id| degree|          department|     school|\n",
      "+---+----------------+---------------+---------------+---+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|              0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|   Matei Zaharia|              1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  2|Michael Armbrust|              1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+----------------+---------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM person LEFT OUTER JOIN graduateProgram ON person.graduate_course = graduateProgram.id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Right Outer Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+---------------+---------------+---+-------+--------------------+-----------+\n",
      "|  id|            name|graduate_course|   spark_status| id| degree|          department|     school|\n",
      "+----+----------------+---------------+---------------+---+-------+--------------------+-----------+\n",
      "|   0|   Bill Chambers|              0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|   1|   Matei Zaharia|              1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|   2|Michael Armbrust|              1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|null|            null|           null|           null|  2|Masters|                EECS|UC Berkeley|\n",
      "+----+----------------+---------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinType = \"right_outer\"\n",
    "\n",
    "joinedDf = person.join(graduateProgram, joinCondition, joinType)\n",
    "joinedDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+---------------+---------------+---+-------+--------------------+-----------+\n",
      "|  id|            name|graduate_course|   spark_status| id| degree|          department|     school|\n",
      "+----+----------------+---------------+---------------+---+-------+--------------------+-----------+\n",
      "|   0|   Bill Chambers|              0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|   1|   Matei Zaharia|              1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|   2|Michael Armbrust|              1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|null|            null|           null|           null|  2|Masters|                EECS|UC Berkeley|\n",
      "+----+----------------+---------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM person RIGHT OUTER JOIN graduateProgram ON person.graduate_course = graduateProgram.id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left Semi Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------+\n",
      "| id| degree|          department|     school|\n",
      "+---+-------+--------------------+-----------+\n",
      "|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinType = \"left_semi\"\n",
    "\n",
    "joinedDf = graduateProgram.join(person, joinCondition, joinType)\n",
    "joinedDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------+\n",
      "| id| degree|          department|     school|\n",
      "+---+-------+--------------------+-----------+\n",
      "|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM graduateProgram LEFT SEMI JOIN person ON graduateProgram.id = person.graduate_course\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left Anti Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+-----------+\n",
      "| id| degree|department|     school|\n",
      "+---+-------+----------+-----------+\n",
      "|  2|Masters|      EECS|UC Berkeley|\n",
      "+---+-------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinType = \"left_anti\"\n",
    "\n",
    "joinedDf = graduateProgram.join(person, joinCondition, joinType)\n",
    "joinedDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+-----------+\n",
      "| id| degree|department|     school|\n",
      "+---+-------+----------+-----------+\n",
      "|  2|Masters|      EECS|UC Berkeley|\n",
      "+---+-------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM graduateProgram LEFT ANTI JOIN person ON graduateProgram.id = person.graduate_course\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+---------------+---------------+---+-------+--------------------+-----------+\n",
      "| id|            name|graduate_course|   spark_status| id| degree|          department|     school|\n",
      "+---+----------------+---------------+---------------+---+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|              0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|   Matei Zaharia|              1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  2|Michael Armbrust|              1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+----------------+---------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinType = \"cross\"\n",
    "\n",
    "joinedDf = person.join(graduateProgram, joinCondition, joinType)\n",
    "joinedDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+---------------+---------------+---+-------+--------------------+-----------+\n",
      "| id|            name|graduate_course|   spark_status| id| degree|          department|     school|\n",
      "+---+----------------+---------------+---------------+---+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|              0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  0|   Bill Chambers|              0|          [100]|  2|Masters|                EECS|UC Berkeley|\n",
      "|  0|   Bill Chambers|              0|          [100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  1|   Matei Zaharia|              1|[500, 250, 100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|   Matei Zaharia|              1|[500, 250, 100]|  2|Masters|                EECS|UC Berkeley|\n",
      "|  1|   Matei Zaharia|              1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  2|Michael Armbrust|              1|     [250, 100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  2|Michael Armbrust|              1|     [250, 100]|  2|Masters|                EECS|UC Berkeley|\n",
      "|  2|Michael Armbrust|              1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+----------------+---------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM person CROSS JOIN graduateProgram\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
