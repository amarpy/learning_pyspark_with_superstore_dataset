{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_excel(\"superstore.xls\")\n",
    "superstore = spark.createDataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Row ID: long (nullable = true)\n",
      " |-- Order ID: string (nullable = true)\n",
      " |-- Order Date: timestamp (nullable = true)\n",
      " |-- Ship Date: timestamp (nullable = true)\n",
      " |-- Ship Mode: string (nullable = true)\n",
      " |-- Customer ID: string (nullable = true)\n",
      " |-- Customer Name: string (nullable = true)\n",
      " |-- Segment: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Postal Code: double (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- Product ID: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Sub-Category: string (nullable = true)\n",
      " |-- Product Name: string (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      " |-- Quantity: long (nullable = true)\n",
      " |-- Discount: double (nullable = true)\n",
      " |-- Profit: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df.printSchema() shows the schema\n",
    "\n",
    "superstore.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|            id|\n",
      "+--------------+\n",
      "|CA-2017-152156|\n",
      "|CA-2017-152156|\n",
      "|CA-2017-138688|\n",
      "|US-2016-108966|\n",
      "|US-2016-108966|\n",
      "+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df.selectExpr(\"`col name` as id\") gives a df with col name renamed in display\n",
    "\n",
    "superstore.selectExpr(\"`Order ID` as id\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+--------+\n",
      "|      Order ID|Postal Code|Quantity|\n",
      "+--------------+-----------+--------+\n",
      "|CA-2015-115259|    43229.0|      14|\n",
      "|CA-2017-145583|    95661.0|      14|\n",
      "|CA-2017-114489|    53132.0|      11|\n",
      "|CA-2017-145625|    92037.0|      13|\n",
      "|CA-2015-122336|    19140.0|      13|\n",
      "+--------------+-----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df.where(\"filter condition\") is used to filter the data based on a given logical condition\n",
    "\n",
    "superstore.where(\"Quantity > 10\")\\\n",
    "          .selectExpr(\"`Order ID`\", \"`Postal Code`\", \"`Quantity`\")\\\n",
    "          .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+--------+\n",
      "|      Order ID|Postal Code|Quantity|\n",
      "+--------------+-----------+--------+\n",
      "|CA-2015-120768|    35630.0|      14|\n",
      "|CA-2018-152702|    61107.0|      14|\n",
      "|CA-2017-145583|    95661.0|      14|\n",
      "|CA-2015-163447|    10011.0|      14|\n",
      "|CA-2016-104241|    22304.0|      14|\n",
      "+--------------+-----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import column\n",
    "\n",
    "#df.sort(col(\"col name\")).desc() orders the dataframe in descending order\n",
    "\n",
    "superstore.where(column(\"Category\") == \"Furniture\")\\\n",
    "          .select(\"Order ID\", \"Postal Code\", \"Quantity\")\\\n",
    "          .sort(column(\"Quantity\").desc())\\\n",
    "          .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+--------+\n",
      "|       Category|Region|Quantity|\n",
      "+---------------+------+--------+\n",
      "|Office Supplies|  West|       2|\n",
      "|Office Supplies|  West|       4|\n",
      "|     Technology|  West|       6|\n",
      "|Office Supplies|  West|       3|\n",
      "|Office Supplies|  West|       5|\n",
      "+---------------+------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#multiple df.where fucntions can be used to filter more conditions\n",
    "\n",
    "superstore.where(column(\"Category\") != \"Furniture\")\\\n",
    "          .where(column(\"Region\") == \"West\")\\\n",
    "          .selectExpr(\"Category\", \"Region\", \"Quantity\")\\\n",
    "          .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+-------+\n",
      "|     col_name|data_type|comment|\n",
      "+-------------+---------+-------+\n",
      "|       Row ID|   bigint|   null|\n",
      "|     Order ID|   string|   null|\n",
      "|   Order Date|timestamp|   null|\n",
      "|    Ship Date|timestamp|   null|\n",
      "|    Ship Mode|   string|   null|\n",
      "|  Customer ID|   string|   null|\n",
      "|Customer Name|   string|   null|\n",
      "|      Segment|   string|   null|\n",
      "|      Country|   string|   null|\n",
      "|         City|   string|   null|\n",
      "|        State|   string|   null|\n",
      "|  Postal Code|   double|   null|\n",
      "|       Region|   string|   null|\n",
      "|   Product ID|   string|   null|\n",
      "|     Category|   string|   null|\n",
      "| Sub-Category|   string|   null|\n",
      "| Product Name|   string|   null|\n",
      "|        Sales|   double|   null|\n",
      "|     Quantity|   bigint|   null|\n",
      "|     Discount|   double|   null|\n",
      "|       Profit|   double|   null|\n",
      "+-------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df.createOrReplaceTempView(\"view name\") can be used to create temp view which can be used to query tables using spark.sql\n",
    "\n",
    "superstore.createOrReplaceTempView(\"superstore_view\")\n",
    "\n",
    "spark.sql(\"\"\"DESCRIBE FORMATTED superstore_view\"\"\").show(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+--------+\n",
      "|       Category|Region|Quantity|\n",
      "+---------------+------+--------+\n",
      "|      Furniture| South|       2|\n",
      "|      Furniture| South|       3|\n",
      "|Office Supplies|  West|       2|\n",
      "|      Furniture| South|       5|\n",
      "|Office Supplies| South|       2|\n",
      "+---------------+------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#spark.sql(\"SQL QUERY\") can be used to query temp views\n",
    "\n",
    "spark.sql(\"SELECT Category, Region, Quantity FROM superstore_view\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Category: string, Region: string, sum(Quantity): bigint]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT  Category, Region, SUM(Quantity)\n",
    "FROM superstore_view \n",
    "GROUP BY Category, Region\n",
    "ORDER BY SUM(Quantity) DESC LIMIT 10\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+\n",
      "|  Category|test|\n",
      "+----------+----+\n",
      "| Furniture|true|\n",
      "| Furniture|true|\n",
      "| Furniture|true|\n",
      "|Technology|true|\n",
      "|Technology|true|\n",
      "+----------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" SELECT Category, (Region = \"Furniture\" AND Quantity > 10 OR SALES > 1000) as test\n",
    "FROM superstore_view WHERE Region = \"Furniture\" AND Quantity > 10 OR SALES > 1000\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>summary</th>\n",
       "      <td>count</td>\n",
       "      <td>mean</td>\n",
       "      <td>stddev</td>\n",
       "      <td>min</td>\n",
       "      <td>max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Row ID</th>\n",
       "      <td>9994</td>\n",
       "      <td>4997.5</td>\n",
       "      <td>2885.1636290974325</td>\n",
       "      <td>1</td>\n",
       "      <td>9994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Order ID</th>\n",
       "      <td>9994</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>CA-2015-100006</td>\n",
       "      <td>US-2018-169551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ship Mode</th>\n",
       "      <td>9994</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>First Class</td>\n",
       "      <td>Standard Class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Customer ID</th>\n",
       "      <td>9994</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>AA-10315</td>\n",
       "      <td>ZD-21925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Customer Name</th>\n",
       "      <td>9994</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Aaron Bergman</td>\n",
       "      <td>Zuschuss Donatelli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Segment</th>\n",
       "      <td>9994</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>Home Office</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Country</th>\n",
       "      <td>9994</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>United States</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>City</th>\n",
       "      <td>9994</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Aberdeen</td>\n",
       "      <td>Yuma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State</th>\n",
       "      <td>9994</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>Wyoming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Postal Code</th>\n",
       "      <td>9994</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Region</th>\n",
       "      <td>9994</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Central</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Product ID</th>\n",
       "      <td>9994</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>FUR-BO-10000112</td>\n",
       "      <td>TEC-PH-10004977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category</th>\n",
       "      <td>9994</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sub-Category</th>\n",
       "      <td>9994</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>Tables</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Product Name</th>\n",
       "      <td>9994</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>\"While you Were Out\" Message Book, One Form pe...</td>\n",
       "      <td>netTALK DUO VoIP Telephone Service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sales</th>\n",
       "      <td>9994</td>\n",
       "      <td>229.85800083049847</td>\n",
       "      <td>623.245100508681</td>\n",
       "      <td>0.44399999999999995</td>\n",
       "      <td>22638.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Quantity</th>\n",
       "      <td>9994</td>\n",
       "      <td>3.789573744246548</td>\n",
       "      <td>2.2251096911414012</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Discount</th>\n",
       "      <td>9994</td>\n",
       "      <td>0.15620272163297735</td>\n",
       "      <td>0.20645196782571612</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Profit</th>\n",
       "      <td>9994</td>\n",
       "      <td>28.6568963077847</td>\n",
       "      <td>234.2601076909574</td>\n",
       "      <td>-6599.978000000001</td>\n",
       "      <td>8399.975999999999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0                    1                    2  \\\n",
       "summary        count                 mean               stddev   \n",
       "Row ID          9994               4997.5   2885.1636290974325   \n",
       "Order ID        9994                 None                 None   \n",
       "Ship Mode       9994                 None                 None   \n",
       "Customer ID     9994                 None                 None   \n",
       "Customer Name   9994                 None                 None   \n",
       "Segment         9994                 None                 None   \n",
       "Country         9994                 None                 None   \n",
       "City            9994                 None                 None   \n",
       "State           9994                 None                 None   \n",
       "Postal Code     9994                  NaN                  NaN   \n",
       "Region          9994                 None                 None   \n",
       "Product ID      9994                 None                 None   \n",
       "Category        9994                 None                 None   \n",
       "Sub-Category    9994                 None                 None   \n",
       "Product Name    9994                 None                 None   \n",
       "Sales           9994   229.85800083049847     623.245100508681   \n",
       "Quantity        9994    3.789573744246548   2.2251096911414012   \n",
       "Discount        9994  0.15620272163297735  0.20645196782571612   \n",
       "Profit          9994     28.6568963077847    234.2601076909574   \n",
       "\n",
       "                                                               3  \\\n",
       "summary                                                      min   \n",
       "Row ID                                                         1   \n",
       "Order ID                                          CA-2015-100006   \n",
       "Ship Mode                                            First Class   \n",
       "Customer ID                                             AA-10315   \n",
       "Customer Name                                      Aaron Bergman   \n",
       "Segment                                                 Consumer   \n",
       "Country                                            United States   \n",
       "City                                                    Aberdeen   \n",
       "State                                                    Alabama   \n",
       "Postal Code                                               1040.0   \n",
       "Region                                                   Central   \n",
       "Product ID                                       FUR-BO-10000112   \n",
       "Category                                               Furniture   \n",
       "Sub-Category                                         Accessories   \n",
       "Product Name   \"While you Were Out\" Message Book, One Form pe...   \n",
       "Sales                                        0.44399999999999995   \n",
       "Quantity                                                       1   \n",
       "Discount                                                     0.0   \n",
       "Profit                                        -6599.978000000001   \n",
       "\n",
       "                                                4  \n",
       "summary                                       max  \n",
       "Row ID                                       9994  \n",
       "Order ID                           US-2018-169551  \n",
       "Ship Mode                          Standard Class  \n",
       "Customer ID                              ZD-21925  \n",
       "Customer Name                  Zuschuss Donatelli  \n",
       "Segment                               Home Office  \n",
       "Country                             United States  \n",
       "City                                         Yuma  \n",
       "State                                     Wyoming  \n",
       "Postal Code                                   NaN  \n",
       "Region                                       West  \n",
       "Product ID                        TEC-PH-10004977  \n",
       "Category                               Technology  \n",
       "Sub-Category                               Tables  \n",
       "Product Name   netTALK DUO VoIP Telephone Service  \n",
       "Sales                                    22638.48  \n",
       "Quantity                                       14  \n",
       "Discount                                      0.8  \n",
       "Profit                          8399.975999999999  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.describe() can be used to display the summary of the dataframe\n",
    "#df.toPandas() can be used to convert a spark dataframe into pandas dataframe\n",
    "\n",
    "superstore.describe().toPandas().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|     State|\n",
      "+----------+\n",
      "|   Alabama|\n",
      "|   Arizona|\n",
      "|  Arkansas|\n",
      "|California|\n",
      "|  Colorado|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df.distinct() gives the distinct values in a selected column\n",
    "\n",
    "superstore.select(\"State\").distinct().sort(\"State\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3002"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.sample() can be used to randomly select a sample of data from a dataframe\n",
    "\n",
    "seed = 5\n",
    "withReplacement = False\n",
    "fraction = 0.3\n",
    "\n",
    "superstore.sample(withReplacement, fraction, seed).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.randomSplit() can be used to split the data into two fractions\n",
    "\n",
    "df = superstore.randomSplit([0.25, 0.75], seed = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test:  2527 \n",
      " Train:  7467\n"
     ]
    }
   ],
   "source": [
    "print(\"Test: \", df[0].count(),\"\\n\",\"Train: \", df[1].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9994"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.union(df2) can be used to join two tables of same columns\n",
    "\n",
    "test = df[0]\n",
    "train = df[1]\n",
    "\n",
    "new = test.union(train)\n",
    "new.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc, asc, col\n",
    "\n",
    "newdf = superstore.select(\"State\").distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|    State|\n",
      "+---------+\n",
      "|     Utah|\n",
      "|Minnesota|\n",
      "|     Ohio|\n",
      "|   Oregon|\n",
      "| Arkansas|\n",
      "+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|     State|\n",
      "+----------+\n",
      "|   Alabama|\n",
      "|   Arizona|\n",
      "|  Arkansas|\n",
      "|California|\n",
      "|  Colorado|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf.sort(\"State\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|        State|\n",
      "+-------------+\n",
      "|      Wyoming|\n",
      "|    Wisconsin|\n",
      "|West Virginia|\n",
      "|   Washington|\n",
      "|     Virginia|\n",
      "+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df.orderBy(col(\"col name\")) can also be used to sort a dataframe column\n",
    "\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "newdf.orderBy(col(\"State\").desc()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "superstore.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "newdf = superstore.repartition(8)\n",
    "\n",
    "print(newdf.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "newdf = newdf.coalesce(2)\n",
    "\n",
    "print(newdf.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "superstore.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|       State|Quantity|\n",
      "+------------+--------+\n",
      "|  California|    7667|\n",
      "|    New York|    4224|\n",
      "|       Texas|    3724|\n",
      "|Pennsylvania|    2153|\n",
      "|    Illinois|    1845|\n",
      "+------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "superstore.where(col(\"State\") != \"Washington\")\\\n",
    ".select(\"State\", \"Quantity\")\\\n",
    ".groupBy(\"State\")\\\n",
    ".sum(\"Quantity\")\\\n",
    ".withColumnRenamed(\"sum(Quantity)\", \"Quantity\")\\\n",
    ".orderBy(col(\"Quantity\").desc())\\\n",
    ".show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+--------+\n",
      "|  State|     Ship Mode|Quantity|\n",
      "+-------+--------------+--------+\n",
      "|Alabama|   First Class|      33|\n",
      "|Alabama|      Same Day|       2|\n",
      "|Alabama|  Second Class|      77|\n",
      "|Alabama|Standard Class|     133|\n",
      "|Arizona|   First Class|     149|\n",
      "+-------+--------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "quantFilter = col(\"Quantity\") > 5\n",
    "catFilter = col(\"Category\") != \"Furniture\"\n",
    "cols = [\"State\", \"Ship Mode\", \"Quantity\"]\n",
    "\n",
    "superstore.where(quantFilter | catFilter)\\\n",
    ".select(\"State\", \"Ship Mode\", \"Quantity\")\\\n",
    ".groupBy(\"State\", \"Ship Mode\")\\\n",
    ".sum(\"Quantity\")\\\n",
    ".withColumnRenamed(\"sum(Quantity)\", \"Quantity\")\\\n",
    ".orderBy(cols, ascending = True)\\\n",
    ".show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|corr(Quantity, Profit)|\n",
      "+----------------------+\n",
      "|   0.06625318912428482|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import corr\n",
    "\n",
    "superstore.stat.corr(\"Quantity\", \"Profit\")\n",
    "superstore.select(corr(\"Quantity\", \"Profit\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------------------+------------------+-------------------+\n",
      "|summary|              Sales|            Profit|          Quantity|           Discount|\n",
      "+-------+-------------------+------------------+------------------+-------------------+\n",
      "|  count|               9994|              9994|              9994|               9994|\n",
      "|   mean| 229.85800083049847|  28.6568963077847| 3.789573744246548|0.15620272163297735|\n",
      "| stddev|   623.245100508681| 234.2601076909574|2.2251096911414012|0.20645196782571612|\n",
      "|    min|0.44399999999999995|-6599.978000000001|                 1|                0.0|\n",
      "|    max|           22638.48| 8399.975999999999|                14|                0.8|\n",
      "+-------+-------------------+------------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "superstore.select(\"Sales\", \"Profit\", \"Quantity\", \"Discount\").describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.31\n",
      "49.848\n",
      "206.112\n"
     ]
    }
   ],
   "source": [
    "colName = \"Sales\"\n",
    "quantileProbs = [0.25, 0.5, 0.75]\n",
    "relError = 0.05\n",
    "\n",
    "for i in superstore.stat.approxQuantile(colName, quantileProbs, relError):\n",
    "    print(round(i, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|ROW_ID|\n",
      "+------+\n",
      "|     0|\n",
      "|     1|\n",
      "|     2|\n",
      "|     3|\n",
      "|     4|\n",
      "+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "superstore.select(monotonically_increasing_id()).withColumnRenamed(\"monotonically_increasing_id()\", \"ROW_ID\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------------+---------------+\n",
      "|initcap(test row)|upper(test row)|lower(TEST ROW)|\n",
      "+-----------------+---------------+---------------+\n",
      "|         Test Row|       TEST ROW|       test row|\n",
      "|         Test Row|       TEST ROW|       test row|\n",
      "|         Test Row|       TEST ROW|       test row|\n",
      "|         Test Row|       TEST ROW|       test row|\n",
      "|         Test Row|       TEST ROW|       test row|\n",
      "+-----------------+---------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import initcap, lower, upper, lit\n",
    "\n",
    "superstore.select(initcap(lit(\"test row\")), upper(lit(\"test row\")), lower(lit(\"TEST ROW\"))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+-----+----------+----+\n",
      "|         ltrim|         rtrim| trim|      rpad|lpad|\n",
      "+--------------+--------------+-----+----------+----+\n",
      "|hello         |         hello|hello|hello     | hel|\n",
      "|hello         |         hello|hello|hello     | hel|\n",
      "|hello         |         hello|hello|hello     | hel|\n",
      "|hello         |         hello|hello|hello     | hel|\n",
      "|hello         |         hello|hello|hello     | hel|\n",
      "+--------------+--------------+-----+----------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import ltrim, rtrim, lpad, rpad, trim\n",
    "\n",
    "superstore.select(ltrim(lit(\"          hello         \")).alias(\"ltrim\"),\n",
    "                 rtrim(lit(\"         hello         \")).alias(\"rtrim\"),\n",
    "                 trim(lit(\"           hello        \")).alias(\"trim\"),\n",
    "                 rpad(lit(\"hello\"), 10, \" \").alias(\"rpad\"),\n",
    "                 lpad(lit(\"hello\"), 3, \" \").alias(\"lpad\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|   latest orders|\n",
      "+----------------+\n",
      "|CA-latest-152156|\n",
      "|CA-latest-152156|\n",
      "|CA-latest-138688|\n",
      "|  US-2016-108966|\n",
      "|  US-2016-108966|\n",
      "+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "regex_string = \"2017|2018\"\n",
    "\n",
    "superstore.select(regexp_replace(col(\"Order ID\"), regex_string, \"latest\").alias(\"latest orders\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|translated state|\n",
      "+----------------+\n",
      "|        K6nt9cky|\n",
      "|        K6nt9cky|\n",
      "|      C5l7f8rn75|\n",
      "|         Fl8r7d5|\n",
      "|         Fl8r7d5|\n",
      "+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import translate\n",
    "\n",
    "superstore.select(translate(col(\"State\"), \"AEIOUaeiou\", \"0123456789\").alias(\"translated state\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------------------+\n",
      "|id |today     |now                    |\n",
      "+---+----------+-----------------------+\n",
      "|0  |2019-03-20|2019-03-20 14:32:46.275|\n",
      "|1  |2019-03-20|2019-03-20 14:32:46.275|\n",
      "|2  |2019-03-20|2019-03-20 14:32:46.275|\n",
      "|3  |2019-03-20|2019-03-20 14:32:46.275|\n",
      "|4  |2019-03-20|2019-03-20 14:32:46.275|\n",
      "+---+----------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_date, current_timestamp\n",
    "\n",
    "dateDF = spark.range(10)\\\n",
    ".withColumn(\"today\", current_date())\\\n",
    ".withColumn(\"now\", current_timestamp())\n",
    "\n",
    "dateDF.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- today: date (nullable = false)\n",
      " |-- now: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dateDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'today', 'now']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dateDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dateDF.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+-----------------------+\n",
      "|date_add(today, 5)|date_sub(today, 5)|date_trunc(yyyy, today)|\n",
      "+------------------+------------------+-----------------------+\n",
      "|        2019-03-25|        2019-03-15|    2019-01-01 00:00:00|\n",
      "|        2019-03-25|        2019-03-15|    2019-01-01 00:00:00|\n",
      "|        2019-03-25|        2019-03-15|    2019-01-01 00:00:00|\n",
      "|        2019-03-25|        2019-03-15|    2019-01-01 00:00:00|\n",
      "|        2019-03-25|        2019-03-15|    2019-01-01 00:00:00|\n",
      "+------------------+------------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_add, date_sub, date_trunc\n",
    "\n",
    "dateDF.select(date_add(col(\"today\"), 5), \n",
    "              date_sub(col(\"today\"), 5), \n",
    "              date_trunc(\"yyyy\", col(\"today\"))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|to_date(`date`)|\n",
      "+---------------+\n",
      "|     2019-01-01|\n",
      "|     2019-01-01|\n",
      "|     2019-01-01|\n",
      "|     2019-01-01|\n",
      "|     2019-01-01|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "spark.range(5).withColumn(\"date\", lit(\"2019-01-01\")).select(to_date(col(\"date\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|datediff(week_ago, today)|\n",
      "+-------------------------+\n",
      "|                       -7|\n",
      "|                       -7|\n",
      "|                       -7|\n",
      "|                       -7|\n",
      "|                       -7|\n",
      "+-------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import datediff, months_between, to_date\n",
    "\n",
    "dateDF.withColumn(\"week_ago\", date_sub(col(\"today\"), 7)).select(datediff(col(\"week_ago\"), col(\"today\"))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|months_between(start, end)|\n",
      "+--------------------------+\n",
      "|               -1.64516129|\n",
      "|               -1.64516129|\n",
      "|               -1.64516129|\n",
      "|               -1.64516129|\n",
      "|               -1.64516129|\n",
      "+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateDF.select(to_date(lit(\"2016-04-01\")).alias(\"start\"),\n",
    "              to_date(lit(\"2016-05-21\")).alias(\"end\"))\\\n",
    "              .select(months_between(col(\"start\"), col(\"end\"))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|months_between(end, start)|\n",
      "+--------------------------+\n",
      "|              333.03225806|\n",
      "|              333.03225806|\n",
      "|              333.03225806|\n",
      "|              333.03225806|\n",
      "|              333.03225806|\n",
      "+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateDF.select(to_date(lit(\"01-04-1991\"), \"dd-MM-yyyy\").alias(\"start\"),\n",
    "             to_date(lit(\"02-01-2019\"), \"dd-MM-yyyy\").alias(\"end\"))\\\n",
    "            .select(months_between(col(\"end\"), col(\"start\"))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|coalesce(City, Product Name)|\n",
      "+----------------------------+\n",
      "|                   Henderson|\n",
      "|                   Henderson|\n",
      "|                 Los Angeles|\n",
      "|             Fort Lauderdale|\n",
      "|             Fort Lauderdale|\n",
      "+----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import coalesce\n",
    "\n",
    "superstore.select(coalesce(col(\"City\"), col(\"Product Name\"))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+-------------------+-------------------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+------------------+--------+--------+-------------------+\n",
      "|Row ID|      Order ID|         Order Date|          Ship Date|     Ship Mode|Customer ID|  Customer Name|  Segment|      Country|           City|     State|Postal Code|Region|     Product ID|       Category|Sub-Category|        Product Name|             Sales|Quantity|Discount|             Profit|\n",
      "+------+--------------+-------------------+-------------------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+------------------+--------+--------+-------------------+\n",
      "|     1|CA-2017-152156|2017-11-08 00:00:00|2017-11-11 00:00:00|  Second Class|   CG-12520|    Claire Gute| Consumer|United States|      Henderson|  Kentucky|    42420.0| South|FUR-BO-10001798|      Furniture|   Bookcases|Bush Somerset Col...|            261.96|       2|     0.0|            41.9136|\n",
      "|     2|CA-2017-152156|2017-11-08 00:00:00|2017-11-11 00:00:00|  Second Class|   CG-12520|    Claire Gute| Consumer|United States|      Henderson|  Kentucky|    42420.0| South|FUR-CH-10000454|      Furniture|      Chairs|Hon Deluxe Fabric...| 731.9399999999999|       3|     0.0| 219.58199999999997|\n",
      "|     3|CA-2017-138688|2017-06-12 00:00:00|2017-06-16 00:00:00|  Second Class|   DV-13045|Darrin Van Huff|Corporate|United States|    Los Angeles|California|    90036.0|  West|OFF-LA-10000240|Office Supplies|      Labels|Self-Adhesive Add...|             14.62|       2|     0.0| 6.8713999999999995|\n",
      "|     4|US-2016-108966|2016-10-11 00:00:00|2016-10-18 00:00:00|Standard Class|   SO-20335| Sean O'Donnell| Consumer|United States|Fort Lauderdale|   Florida|    33311.0| South|FUR-TA-10000577|      Furniture|      Tables|Bretford CR4500 S...|          957.5775|       5|    0.45|-383.03100000000006|\n",
      "|     5|US-2016-108966|2016-10-11 00:00:00|2016-10-18 00:00:00|Standard Class|   SO-20335| Sean O'Donnell| Consumer|United States|Fort Lauderdale|   Florida|    33311.0| South|OFF-ST-10000760|Office Supplies|     Storage|Eldon Fold 'N Rol...|22.368000000000002|       2|     0.2|  2.516399999999999|\n",
      "+------+--------------+-------------------+-------------------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+------------------+--------+--------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "superstore.na.drop().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+-------------------+-------------------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+------------------+--------+--------+-------------------+\n",
      "|Row ID|      Order ID|         Order Date|          Ship Date|     Ship Mode|Customer ID|  Customer Name|  Segment|      Country|           City|     State|Postal Code|Region|     Product ID|       Category|Sub-Category|        Product Name|             Sales|Quantity|Discount|             Profit|\n",
      "+------+--------------+-------------------+-------------------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+------------------+--------+--------+-------------------+\n",
      "|     1|CA-2017-152156|2017-11-08 00:00:00|2017-11-11 00:00:00|  Second Class|   CG-12520|    Claire Gute| Consumer|United States|      Henderson|  Kentucky|    42420.0| South|FUR-BO-10001798|      Furniture|   Bookcases|Bush Somerset Col...|            261.96|       2|     0.0|            41.9136|\n",
      "|     2|CA-2017-152156|2017-11-08 00:00:00|2017-11-11 00:00:00|  Second Class|   CG-12520|    Claire Gute| Consumer|United States|      Henderson|  Kentucky|    42420.0| South|FUR-CH-10000454|      Furniture|      Chairs|Hon Deluxe Fabric...| 731.9399999999999|       3|     0.0| 219.58199999999997|\n",
      "|     3|CA-2017-138688|2017-06-12 00:00:00|2017-06-16 00:00:00|  Second Class|   DV-13045|Darrin Van Huff|Corporate|United States|    Los Angeles|California|    90036.0|  West|OFF-LA-10000240|Office Supplies|      Labels|Self-Adhesive Add...|             14.62|       2|     0.0| 6.8713999999999995|\n",
      "|     4|US-2016-108966|2016-10-11 00:00:00|2016-10-18 00:00:00|Standard Class|   SO-20335| Sean O'Donnell| Consumer|United States|Fort Lauderdale|   Florida|    33311.0| South|FUR-TA-10000577|      Furniture|      Tables|Bretford CR4500 S...|          957.5775|       5|    0.45|-383.03100000000006|\n",
      "|     5|US-2016-108966|2016-10-11 00:00:00|2016-10-18 00:00:00|Standard Class|   SO-20335| Sean O'Donnell| Consumer|United States|Fort Lauderdale|   Florida|    33311.0| South|OFF-ST-10000760|Office Supplies|     Storage|Eldon Fold 'N Rol...|22.368000000000002|       2|     0.2|  2.516399999999999|\n",
      "+------+--------------+-------------------+-------------------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+------------------+--------+--------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "superstore.na.drop(\"all\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+-------------------+-------------------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+------------------+--------+--------+-------------------+\n",
      "|Row ID|      Order ID|         Order Date|          Ship Date|     Ship Mode|Customer ID|  Customer Name|  Segment|      Country|           City|     State|Postal Code|Region|     Product ID|       Category|Sub-Category|        Product Name|             Sales|Quantity|Discount|             Profit|\n",
      "+------+--------------+-------------------+-------------------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+------------------+--------+--------+-------------------+\n",
      "|     1|CA-2017-152156|2017-11-08 00:00:00|2017-11-11 00:00:00|  Second Class|   CG-12520|    Claire Gute| Consumer|United States|      Henderson|  Kentucky|    42420.0| South|FUR-BO-10001798|      Furniture|   Bookcases|Bush Somerset Col...|            261.96|       2|     0.0|            41.9136|\n",
      "|     2|CA-2017-152156|2017-11-08 00:00:00|2017-11-11 00:00:00|  Second Class|   CG-12520|    Claire Gute| Consumer|United States|      Henderson|  Kentucky|    42420.0| South|FUR-CH-10000454|      Furniture|      Chairs|Hon Deluxe Fabric...| 731.9399999999999|       3|     0.0| 219.58199999999997|\n",
      "|     3|CA-2017-138688|2017-06-12 00:00:00|2017-06-16 00:00:00|  Second Class|   DV-13045|Darrin Van Huff|Corporate|United States|    Los Angeles|California|    90036.0|  West|OFF-LA-10000240|Office Supplies|      Labels|Self-Adhesive Add...|             14.62|       2|     0.0| 6.8713999999999995|\n",
      "|     4|US-2016-108966|2016-10-11 00:00:00|2016-10-18 00:00:00|Standard Class|   SO-20335| Sean O'Donnell| Consumer|United States|Fort Lauderdale|   Florida|    33311.0| South|FUR-TA-10000577|      Furniture|      Tables|Bretford CR4500 S...|          957.5775|       5|    0.45|-383.03100000000006|\n",
      "|     5|US-2016-108966|2016-10-11 00:00:00|2016-10-18 00:00:00|Standard Class|   SO-20335| Sean O'Donnell| Consumer|United States|Fort Lauderdale|   Florida|    33311.0| South|OFF-ST-10000760|Office Supplies|     Storage|Eldon Fold 'N Rol...|22.368000000000002|       2|     0.2|  2.516399999999999|\n",
      "+------+--------------+-------------------+-------------------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+------------------+--------+--------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "superstore.na.drop(\"all\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+-------------------+-------------------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+------------------+--------+--------+-------------------+\n",
      "|Row ID|      Order ID|         Order Date|          Ship Date|     Ship Mode|Customer ID|  Customer Name|  Segment|      Country|           City|     State|Postal Code|Region|     Product ID|       Category|Sub-Category|        Product Name|             Sales|Quantity|Discount|             Profit|\n",
      "+------+--------------+-------------------+-------------------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+------------------+--------+--------+-------------------+\n",
      "|     1|CA-2017-152156|2017-11-08 00:00:00|2017-11-11 00:00:00|  Second Class|   CG-12520|    Claire Gute| Consumer|United States|      Henderson|  Kentucky|    42420.0| South|FUR-BO-10001798|      Furniture|   Bookcases|Bush Somerset Col...|            261.96|       2|     0.0|            41.9136|\n",
      "|     2|CA-2017-152156|2017-11-08 00:00:00|2017-11-11 00:00:00|  Second Class|   CG-12520|    Claire Gute| Consumer|United States|      Henderson|  Kentucky|    42420.0| South|FUR-CH-10000454|      Furniture|      Chairs|Hon Deluxe Fabric...| 731.9399999999999|       3|     0.0| 219.58199999999997|\n",
      "|     3|CA-2017-138688|2017-06-12 00:00:00|2017-06-16 00:00:00|  Second Class|   DV-13045|Darrin Van Huff|Corporate|United States|    Los Angeles|California|    90036.0|  West|OFF-LA-10000240|Office Supplies|      Labels|Self-Adhesive Add...|             14.62|       2|     0.0| 6.8713999999999995|\n",
      "|     4|US-2016-108966|2016-10-11 00:00:00|2016-10-18 00:00:00|Standard Class|   SO-20335| Sean O'Donnell| Consumer|United States|Fort Lauderdale|   Florida|    33311.0| South|FUR-TA-10000577|      Furniture|      Tables|Bretford CR4500 S...|          957.5775|       5|    0.45|-383.03100000000006|\n",
      "|     5|US-2016-108966|2016-10-11 00:00:00|2016-10-18 00:00:00|Standard Class|   SO-20335| Sean O'Donnell| Consumer|United States|Fort Lauderdale|   Florida|    33311.0| South|OFF-ST-10000760|Office Supplies|     Storage|Eldon Fold 'N Rol...|22.368000000000002|       2|     0.2|  2.516399999999999|\n",
      "+------+--------------+-------------------+-------------------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+------------------+--------+--------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "superstore.na.drop(\"all\", subset = [\"Order Id\", \"Order Date\"]).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+-------------------+-------------------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+------------------+--------+--------+-------------------+\n",
      "|Row ID|      Order ID|         Order Date|          Ship Date|     Ship Mode|Customer ID|  Customer Name|  Segment|      Country|           City|     State|Postal Code|Region|     Product ID|       Category|Sub-Category|        Product Name|             Sales|Quantity|Discount|             Profit|\n",
      "+------+--------------+-------------------+-------------------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+------------------+--------+--------+-------------------+\n",
      "|     1|CA-2017-152156|2017-11-08 00:00:00|2017-11-11 00:00:00|  Second Class|   CG-12520|    Claire Gute| Consumer|United States|      Henderson|  Kentucky|    42420.0| South|FUR-BO-10001798|      Furniture|   Bookcases|Bush Somerset Col...|            261.96|       2|     0.0|            41.9136|\n",
      "|     2|CA-2017-152156|2017-11-08 00:00:00|2017-11-11 00:00:00|  Second Class|   CG-12520|    Claire Gute| Consumer|United States|      Henderson|  Kentucky|    42420.0| South|FUR-CH-10000454|      Furniture|      Chairs|Hon Deluxe Fabric...| 731.9399999999999|       3|     0.0| 219.58199999999997|\n",
      "|     3|CA-2017-138688|2017-06-12 00:00:00|2017-06-16 00:00:00|  Second Class|   DV-13045|Darrin Van Huff|Corporate|United States|    Los Angeles|California|    90036.0|  West|OFF-LA-10000240|Office Supplies|      Labels|Self-Adhesive Add...|             14.62|       2|     0.0| 6.8713999999999995|\n",
      "|     4|US-2016-108966|2016-10-11 00:00:00|2016-10-18 00:00:00|Standard Class|   SO-20335| Sean O'Donnell| Consumer|United States|Fort Lauderdale|   Florida|    33311.0| South|FUR-TA-10000577|      Furniture|      Tables|Bretford CR4500 S...|          957.5775|       5|    0.45|-383.03100000000006|\n",
      "|     5|US-2016-108966|2016-10-11 00:00:00|2016-10-18 00:00:00|Standard Class|   SO-20335| Sean O'Donnell| Consumer|United States|Fort Lauderdale|   Florida|    33311.0| South|OFF-ST-10000760|Office Supplies|     Storage|Eldon Fold 'N Rol...|22.368000000000002|       2|     0.2|  2.516399999999999|\n",
      "+------+--------------+-------------------+-------------------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+------------------+--------+--------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "superstore.na.fill(\"All null values become this string\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+-------------------+-------------------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+------------------+--------+--------+-------------------+\n",
      "|Row ID|      Order ID|         Order Date|          Ship Date|     Ship Mode|Customer ID|  Customer Name|  Segment|      Country|           City|     State|Postal Code|Region|     Product ID|       Category|Sub-Category|        Product Name|             Sales|Quantity|Discount|             Profit|\n",
      "+------+--------------+-------------------+-------------------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+------------------+--------+--------+-------------------+\n",
      "|     1|CA-2017-152156|2017-11-08 00:00:00|2017-11-11 00:00:00|  Second Class|   CG-12520|    Claire Gute| Consumer|United States|      Henderson|  Kentucky|    42420.0| South|FUR-BO-10001798|      Furniture|   Bookcases|Bush Somerset Col...|            261.96|       2|     0.0|            41.9136|\n",
      "|     2|CA-2017-152156|2017-11-08 00:00:00|2017-11-11 00:00:00|  Second Class|   CG-12520|    Claire Gute| Consumer|United States|      Henderson|  Kentucky|    42420.0| South|FUR-CH-10000454|      Furniture|      Chairs|Hon Deluxe Fabric...| 731.9399999999999|       3|     0.0| 219.58199999999997|\n",
      "|     3|CA-2017-138688|2017-06-12 00:00:00|2017-06-16 00:00:00|  Second Class|   DV-13045|Darrin Van Huff|Corporate|United States|    Los Angeles|California|    90036.0|  West|OFF-LA-10000240|Office Supplies|      Labels|Self-Adhesive Add...|             14.62|       2|     0.0| 6.8713999999999995|\n",
      "|     4|US-2016-108966|2016-10-11 00:00:00|2016-10-18 00:00:00|Standard Class|   SO-20335| Sean O'Donnell| Consumer|United States|Fort Lauderdale|   Florida|    33311.0| South|FUR-TA-10000577|      Furniture|      Tables|Bretford CR4500 S...|          957.5775|       5|    0.45|-383.03100000000006|\n",
      "|     5|US-2016-108966|2016-10-11 00:00:00|2016-10-18 00:00:00|Standard Class|   SO-20335| Sean O'Donnell| Consumer|United States|Fort Lauderdale|   Florida|    33311.0| South|OFF-ST-10000760|Office Supplies|     Storage|Eldon Fold 'N Rol...|22.368000000000002|       2|     0.2|  2.516399999999999|\n",
      "+------+--------------+-------------------+-------------------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+------------------+--------+--------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "superstore.na.replace([\" \"], [\"UNKNOWN\"], \"Description\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#struct funtion is used to create a complex column by combining multiple columns so that they can be later queried\n",
    "\n",
    "from pyspark.sql.functions import struct\n",
    "\n",
    "df_test = superstore.select(struct(\"Row ID\", \"Order ID\").alias(\"complex\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|      Order ID|\n",
      "+--------------+\n",
      "|CA-2017-152156|\n",
      "|CA-2017-152156|\n",
      "|CA-2017-138688|\n",
      "|US-2016-108966|\n",
      "|US-2016-108966|\n",
      "+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.select(\"complex.Order ID\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|First_and_Last_Names[1]|\n",
      "+-----------------------+\n",
      "|                   Gute|\n",
      "|                   Gute|\n",
      "|                    Van|\n",
      "|              O'Donnell|\n",
      "|              O'Donnell|\n",
      "+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#split function is used to split rows of a column into arrays\n",
    "\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "superstore.select(split(col(\"Customer Name\"), \" \").alias(\"First_and_Last_Names\"))\\\n",
    ".selectExpr(\"First_and_Last_Names[1]\")\\\n",
    ".show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|name_split|\n",
      "+----------+\n",
      "|         2|\n",
      "|         2|\n",
      "|         3|\n",
      "|         2|\n",
      "|         2|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#size function can be used to find the size of the array\n",
    "\n",
    "from pyspark.sql.functions import size, array_contains\n",
    "\n",
    "superstore.select(size(split(col(\"Customer Name\"), \" \")).alias(\"name_split\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|is_hoffman|\n",
      "+----------+\n",
      "|     false|\n",
      "|     false|\n",
      "|     false|\n",
      "|     false|\n",
      "|     false|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#array_contains can be used to check whether the array contains a given value\n",
    "\n",
    "superstore.select(array_contains(split(col(\"Customer Name\"), \" \"), \"Hoffman\").alias(\"is_hoffman\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+--------+\n",
      "|  Customer Name|           splitted|exploded|\n",
      "+---------------+-------------------+--------+\n",
      "|    Claire Gute|     [Claire, Gute]|  Claire|\n",
      "|    Claire Gute|     [Claire, Gute]|    Gute|\n",
      "|    Claire Gute|     [Claire, Gute]|  Claire|\n",
      "|    Claire Gute|     [Claire, Gute]|    Gute|\n",
      "|Darrin Van Huff|[Darrin, Van, Huff]|  Darrin|\n",
      "+---------------+-------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#explode function can be used to create new rows from the indicidual values of an array\n",
    "\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "superstore.withColumn(\"splitted\", split(col(\"Customer Name\"), \" \"))\\\n",
    ".withColumn(\"exploded\", explode(col(\"splitted\")))\\\n",
    ".select(\"Customer Name\",\"splitted\", \"exploded\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n",
      "|mapped                             |\n",
      "+-----------------------------------+\n",
      "|[Claire Gute -> CA-2017-152156]    |\n",
      "|[Claire Gute -> CA-2017-152156]    |\n",
      "|[Darrin Van Huff -> CA-2017-138688]|\n",
      "|[Sean O'Donnell -> US-2016-108966] |\n",
      "|[Sean O'Donnell -> US-2016-108966] |\n",
      "+-----------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#map function can be used to create key value pairs of columns\n",
    "\n",
    "from pyspark.sql.functions import create_map\n",
    "\n",
    "superstore.select(create_map(col(\"Customer Name\"), col(\"Order ID\")).alias(\"mapped\")).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|mapped[Claire Gute]|\n",
      "+-------------------+\n",
      "|     CA-2017-152156|\n",
      "|     CA-2017-152156|\n",
      "|               null|\n",
      "|               null|\n",
      "|               null|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#maps can be queried\n",
    "\n",
    "superstore.select(create_map(col(\"Customer Name\"), col(\"Order ID\")).alias(\"mapped\"))\\\n",
    ".selectExpr(\"mapped['Claire Gute']\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#handling jason data\n",
    "\n",
    "jsondf = spark.range(1).selectExpr(\"\"\" '{\"myJsonKey\": \n",
    "                                                {\"myJsonValues\": [1, 2, 3]}}' \n",
    "                                                    as jsonString \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------------+\n",
      "|column|jsonKey                 |\n",
      "+------+------------------------+\n",
      "|1     |{\"myJsonValues\":[1,2,3]}|\n",
      "+------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import get_json_object, json_tuple\n",
    "\n",
    "jsondf.select(\n",
    "    get_json_object(col(\"jsonString\"), \"$.myJsonKey.myJsonValues[0]\").alias(\"column\"), \n",
    "    json_tuple(col(\"jsonString\"), \"myJsonKey\").alias(\"jsonKey\")).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------+\n",
      "|structstojson(myStruct)                                        |\n",
      "+---------------------------------------------------------------+\n",
      "|{\"Order ID\":\"CA-2017-152156\",\"Customer Name\":\"Claire Gute\"}    |\n",
      "|{\"Order ID\":\"CA-2017-152156\",\"Customer Name\":\"Claire Gute\"}    |\n",
      "|{\"Order ID\":\"CA-2017-138688\",\"Customer Name\":\"Darrin Van Huff\"}|\n",
      "|{\"Order ID\":\"US-2016-108966\",\"Customer Name\":\"Sean O'Donnell\"} |\n",
      "|{\"Order ID\":\"US-2016-108966\",\"Customer Name\":\"Sean O'Donnell\"} |\n",
      "+---------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_json\n",
    "\n",
    "superstore.selectExpr(\"(`Order ID`, `Customer Name`) as myStruct\")\\\n",
    ".select(to_json(col(\"myStruct\"))).show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|count(Order ID)|\n",
      "+---------------+\n",
      "|           9994|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Count of the Order IDs\n",
    "\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "superstore.select(count(\"Order ID\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|count(DISTINCT Order ID)|\n",
      "+------------------------+\n",
      "|                    5009|\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Count of distinct Order ID\n",
    "\n",
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "superstore.select(countDistinct(\"Order ID\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+\n",
      "|approx_count_distinct(Order ID)|\n",
      "+-------------------------------+\n",
      "|                           4967|\n",
      "+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#find only a certain degree of of count distinct\n",
    "\n",
    "from pyspark.sql.functions import approx_count_distinct\n",
    "\n",
    "superstore.select(approx_count_distinct(\"Order ID\", 0.01)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+---------------------+\n",
      "|first(Order ID, false)|last(Order ID, false)|\n",
      "+----------------------+---------------------+\n",
      "|        CA-2017-152156|       CA-2018-119914|\n",
      "+----------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#find the first and the last items in a columns\n",
    "\n",
    "from pyspark.sql.functions import first, last\n",
    "\n",
    "superstore.select(first(\"Order ID\"), last(\"Order ID\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|min(Quantity)|max(Quantity)|\n",
      "+-------------+-------------+\n",
      "|            1|           14|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#find the min and max for a column\n",
    "\n",
    "from pyspark.sql.functions import min, max\n",
    "\n",
    "superstore.select(min(\"Quantity\"), max(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|        sum(Sales)|\n",
      "+------------------+\n",
      "|2297200.8603000017|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#find the sum of a numerical column\n",
    "\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "superstore.select(sum(\"Sales\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|sum(DISTINCT Sales)|\n",
      "+-------------------+\n",
      "| 1831956.4445000002|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find the disticnt sum of a column\n",
    "\n",
    "from pyspark.sql.functions import sumDistinct\n",
    "\n",
    "superstore.select(sumDistinct(\"Sales\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------------------+------------------+------------------+\n",
      "|total_products_sold|distinct_products_sold|total_sales_amount|        mean_sales|\n",
      "+-------------------+----------------------+------------------+------------------+\n",
      "|              37873|                   793|2297200.8603000003|229.85800083049833|\n",
      "+-------------------+----------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#find the sum, max, min, expr of a column\n",
    "\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "superstore.select(sum(\"Quantity\").alias(\"total_products_sold\"), \n",
    "                  countDistinct(\"Customer ID\").alias(\"distinct_products_sold\"),\n",
    "                 sum(\"Sales\").alias(\"total_sales_amount\"),\n",
    "                 avg(\"Sales\").alias(\"mean_sales\")).selectExpr(\"total_products_sold\", \n",
    "                                                             \"distinct_products_sold\",\n",
    "                                                             \"total_sales_amount\",\n",
    "                                                             \"mean_sales\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+--------------------+---------------------+\n",
      "|var_pop(Quantity)|var_samp(Quantity)|stddev_pop(Quantity)|stddev_samp(Quantity)|\n",
      "+-----------------+------------------+--------------------+---------------------+\n",
      "|4.950617729052486| 4.951113137611382|  2.2249983660786103|   2.2251096911414012|\n",
      "+-----------------+------------------+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import var_pop, stddev_pop, var_samp, stddev_samp\n",
    "\n",
    "superstore.select(var_pop(\"Quantity\"), var_samp(\"Quantity\"), \n",
    "                  stddev_pop(\"Quantity\"), stddev_samp(\"Quantity\")).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------+\n",
      "|   skewness(Sales)|  kurtosis(Sales)|\n",
      "+------------------+-----------------+\n",
      "|12.970805179533508|305.1584268174984|\n",
      "+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import skewness, kurtosis\n",
    "\n",
    "superstore.select(skewness(\"Sales\"), kurtosis(\"Sales\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "| collect_list(State)|  collect_set(State)|\n",
      "+--------------------+--------------------+\n",
      "|[Kentucky, Kentuc...|[Michigan, Vermon...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#aggregarte complex types e.g aggregate the number of states\n",
    "\n",
    "from pyspark.sql.functions import collect_list, collect_set\n",
    "\n",
    "superstore.agg(collect_list(\"State\"), collect_set(\"State\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----------+\n",
      "| Region|               State|Order Count|\n",
      "+-------+--------------------+-----------+\n",
      "|Central|            Illinois|        492|\n",
      "|Central|             Indiana|        149|\n",
      "|Central|                Iowa|         30|\n",
      "|Central|              Kansas|         24|\n",
      "|Central|            Michigan|        255|\n",
      "|Central|           Minnesota|         89|\n",
      "|Central|            Missouri|         66|\n",
      "|Central|            Nebraska|         38|\n",
      "|Central|        North Dakota|          7|\n",
      "|Central|            Oklahoma|         66|\n",
      "|Central|        South Dakota|         12|\n",
      "|Central|               Texas|        985|\n",
      "|Central|           Wisconsin|        110|\n",
      "|   East|         Connecticut|         82|\n",
      "|   East|            Delaware|         96|\n",
      "|   East|District of Columbia|         10|\n",
      "|   East|               Maine|          8|\n",
      "|   East|            Maryland|        105|\n",
      "|   East|       Massachusetts|        135|\n",
      "|   East|       New Hampshire|         27|\n",
      "|   East|          New Jersey|        130|\n",
      "|   East|            New York|       1128|\n",
      "|   East|                Ohio|        469|\n",
      "|   East|        Pennsylvania|        587|\n",
      "|   East|        Rhode Island|         56|\n",
      "|   East|             Vermont|         11|\n",
      "|   East|       West Virginia|          4|\n",
      "|  South|             Alabama|         61|\n",
      "|  South|            Arkansas|         60|\n",
      "|  South|             Florida|        383|\n",
      "|  South|             Georgia|        184|\n",
      "|  South|            Kentucky|        139|\n",
      "|  South|           Louisiana|         42|\n",
      "|  South|         Mississippi|         53|\n",
      "|  South|      North Carolina|        249|\n",
      "|  South|      South Carolina|         42|\n",
      "|  South|           Tennessee|        183|\n",
      "|  South|            Virginia|        224|\n",
      "|   West|             Arizona|        224|\n",
      "|   West|          California|       2001|\n",
      "|   West|            Colorado|        182|\n",
      "|   West|               Idaho|         21|\n",
      "|   West|             Montana|         15|\n",
      "|   West|              Nevada|         39|\n",
      "|   West|          New Mexico|         37|\n",
      "|   West|              Oregon|        124|\n",
      "|   West|                Utah|         53|\n",
      "|   West|          Washington|        506|\n",
      "|   West|             Wyoming|          1|\n",
      "+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#groupby condition on dataframes\n",
    "\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "superstore.groupby(\"Region\", \"State\").agg(expr(\"count('Order ID')\").alias(\"Order Count\"))\\\n",
    ".orderBy(\"Region\", \"State\").show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------+--------------------+\n",
      "|      Order ID|max_quantity|stddev_pop(Quantity)|\n",
      "+--------------+------------+--------------------+\n",
      "|US-2016-119312|          14|                 0.0|\n",
      "|CA-2017-145583|          14|   4.060762972443398|\n",
      "|CA-2015-154599|          14|   4.301162633521313|\n",
      "|CA-2015-120768|          14|   5.354126134736337|\n",
      "|CA-2016-104241|          14|                 0.0|\n",
      "|CA-2016-149713|          14|  5.2493385826745405|\n",
      "|CA-2017-142405|          14|   5.436502143433363|\n",
      "|CA-2018-152702|          14|                 0.0|\n",
      "|US-2017-103674|          14|   3.833259389999639|\n",
      "|CA-2018-130036|          14|                 5.5|\n",
      "|CA-2018-161410|          14|                 5.5|\n",
      "|CA-2018-164028|          14|                 0.0|\n",
      "|CA-2017-105732|          14|  3.3823069050575527|\n",
      "|CA-2015-158337|          14|                 0.0|\n",
      "|CA-2015-154165|          14|                 0.0|\n",
      "|CA-2016-103135|          14|    4.06201920231798|\n",
      "|CA-2017-140571|          14|                 6.0|\n",
      "|CA-2018-169859|          14|   4.784233364802441|\n",
      "|US-2016-164448|          14|                 4.5|\n",
      "|CA-2018-151750|          14|  3.5799897388976194|\n",
      "+--------------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#group by functions \n",
    "\n",
    "superstore.groupby(\"Order ID\").agg(expr(\"max(Quantity)\").alias(\"max_quantity\"), stddev_pop(\"Quantity\"))\\\n",
    ".orderBy(\"max_quantity\", ascending = False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#window functions ***IMPORTANT***\n",
    "\n",
    "dfWithDate = superstore.withColumn(\"date\", to_date(col(\"Order Date\"), \"MM/d/yyyy H:mm\"))\n",
    "dfWithDate = dfWithDate.select(col(\"Category\"), col(\"date\"), col(\"Sales\"))\n",
    "\n",
    "dfWithDate.createOrReplaceTempView(\"dfWithDate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+-----------------+\n",
      "|       Category|      date|            Sales|\n",
      "+---------------+----------+-----------------+\n",
      "|      Furniture|2017-11-08|           261.96|\n",
      "|      Furniture|2017-11-08|731.9399999999999|\n",
      "|Office Supplies|2017-06-12|            14.62|\n",
      "+---------------+----------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithDate.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "windowSpec = Window\\\n",
    ".partitionBy(\"date\", \"Category\")\\\n",
    ".orderBy(desc(\"Sales\"))\\\n",
    ".rowsBetween(Window.unboundedPreceding, Window.currentRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxPurchaseQuantity = max(col(\"Sales\")).over(windowSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import dense_rank, rank, row_number\n",
    "\n",
    "purchaseDenseRank = dense_rank().over(windowSpec)\n",
    "purchaseRank = rank().over(windowSpec)\n",
    "rownum = row_number().over(windowSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+------------------+----+----------+------------------+------+\n",
      "|       Category|      date|             Sales|rank|dense_rank|         max_sales|row id|\n",
      "+---------------+----------+------------------+----+----------+------------------+------+\n",
      "|Office Supplies|2015-01-03|            16.448|   1|         1|            16.448|     1|\n",
      "|Office Supplies|2015-01-04|           272.736|   1|         1|           272.736|     1|\n",
      "|Office Supplies|2015-01-04|            11.784|   2|         2|           272.736|     2|\n",
      "|Office Supplies|2015-01-04| 3.539999999999999|   3|         3|           272.736|     3|\n",
      "|Office Supplies|2015-01-05|            19.536|   1|         1|            19.536|     1|\n",
      "|      Furniture|2015-01-06|           2573.82|   1|         1|           2573.82|     1|\n",
      "|Office Supplies|2015-01-06|            609.98|   1|         1|            609.98|     1|\n",
      "|Office Supplies|2015-01-06|             31.12|   2|         2|            609.98|     2|\n",
      "|Office Supplies|2015-01-06|             19.44|   3|         3|            609.98|     3|\n",
      "|Office Supplies|2015-01-06|             12.78|   4|         4|            609.98|     4|\n",
      "|Office Supplies|2015-01-06|              6.54|   5|         5|            609.98|     5|\n",
      "|Office Supplies|2015-01-06|              5.48|   6|         6|            609.98|     6|\n",
      "|     Technology|2015-01-06|            755.96|   1|         1|            755.96|     1|\n",
      "|     Technology|2015-01-06|            391.98|   2|         2|            755.96|     2|\n",
      "|      Furniture|2015-01-07| 76.72800000000001|   1|         1| 76.72800000000001|     1|\n",
      "|Office Supplies|2015-01-07|10.429999999999998|   1|         1|10.429999999999998|     1|\n",
      "|Office Supplies|2015-01-09|             9.344|   1|         1|             9.344|     1|\n",
      "|     Technology|2015-01-09|31.200000000000003|   1|         1|31.200000000000003|     1|\n",
      "|      Furniture|2015-01-10|             51.94|   1|         1|             51.94|     1|\n",
      "|Office Supplies|2015-01-10|              2.89|   1|         1|              2.89|     1|\n",
      "+---------------+----------+------------------+----+----------+------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithDate.where(\"`Category` IS NOT NULL\").orderBy(\"date\")\\\n",
    ".select(col(\"Category\"), \n",
    "        col(\"date\"), \n",
    "        col(\"Sales\"), \n",
    "        purchaseRank.alias(\"rank\"), \n",
    "        purchaseDenseRank.alias(\"dense_rank\"), \n",
    "        maxPurchaseQuantity.alias(\"max_sales\"),\n",
    "       rownum.alias(\"row id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#roll up\n",
    "\n",
    "dfNoNull = dfWithDate.drop()\n",
    "dfNoNull.createOrReplaceTempView(\"dfNoNull\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+------------------+\n",
      "|       Category|      date|             Sales|\n",
      "+---------------+----------+------------------+\n",
      "|      Furniture|2017-11-08|            261.96|\n",
      "|      Furniture|2017-11-08| 731.9399999999999|\n",
      "|Office Supplies|2017-06-12|             14.62|\n",
      "|      Furniture|2016-10-11|          957.5775|\n",
      "|Office Supplies|2016-10-11|22.368000000000002|\n",
      "+---------------+----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfNoNull.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+------------------+\n",
      "| Category|      date|        sum(Sales)|\n",
      "+---------+----------+------------------+\n",
      "|     null|      null|2297200.8603000017|\n",
      "|Furniture|2018-12-30|           323.136|\n",
      "|Furniture|2018-12-29|          2330.718|\n",
      "|Furniture|2018-12-28| 551.2568000000001|\n",
      "|Furniture|2018-12-25|           832.454|\n",
      "|Furniture|2018-12-24|1393.4940000000001|\n",
      "|Furniture|2018-12-23|           282.114|\n",
      "|Furniture|2018-12-22| 4086.455999999999|\n",
      "|Furniture|2018-12-21|             15.92|\n",
      "|Furniture|2018-12-19|115.37800000000001|\n",
      "+---------+----------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#grouping sets can be used to group data across multiple groups. Only available in SQL\n",
    "\n",
    "spark.sql(\"SELECT Category, date, sum(Sales) FROM dfNoNull GROUP BY Category, date GROUPING SETS ((Category, date), ()) ORDER BY Category ASC, date DESC\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+------------------+\n",
      "|      date|       Category|       total_sales|\n",
      "+----------+---------------+------------------+\n",
      "|      null|           null|2297200.8603000017|\n",
      "|2015-01-03|Office Supplies|            16.448|\n",
      "|2015-01-03|           null|            16.448|\n",
      "|2015-01-04|           null|            288.06|\n",
      "|2015-01-04|Office Supplies|            288.06|\n",
      "|2015-01-05|Office Supplies|            19.536|\n",
      "|2015-01-05|           null|            19.536|\n",
      "|2015-01-06|           null| 4407.099999999999|\n",
      "|2015-01-06|      Furniture|           2573.82|\n",
      "|2015-01-06|Office Supplies|            685.34|\n",
      "|2015-01-06|     Technology|           1147.94|\n",
      "|2015-01-07|Office Supplies|10.429999999999998|\n",
      "|2015-01-07|      Furniture| 76.72800000000001|\n",
      "|2015-01-07|           null|            87.158|\n",
      "|2015-01-09|Office Supplies|             9.344|\n",
      "|2015-01-09|     Technology|31.200000000000003|\n",
      "|2015-01-09|           null|40.544000000000004|\n",
      "|2015-01-10|           null|             54.83|\n",
      "|2015-01-10|Office Supplies|              2.89|\n",
      "|2015-01-10|      Furniture|             51.94|\n",
      "+----------+---------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#roll up function\n",
    "\n",
    "rolledUpDF = dfNoNull.rollup(\"date\", \"Category\")\\\n",
    ".agg(sum(\"Sales\")).selectExpr(\"date\", \"Category\", \"`sum(Sales)` as total_sales\")\\\n",
    ".orderBy(\"date\")\n",
    "\n",
    "rolledUpDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+------------------+\n",
      "|      date|Category|       total_sales|\n",
      "+----------+--------+------------------+\n",
      "|      null|    null|2297200.8603000017|\n",
      "|2015-01-03|    null|            16.448|\n",
      "|2015-01-04|    null|            288.06|\n",
      "|2015-01-05|    null|            19.536|\n",
      "|2015-01-06|    null| 4407.099999999999|\n",
      "|2015-01-07|    null|            87.158|\n",
      "|2015-01-09|    null|40.544000000000004|\n",
      "|2015-01-10|    null|             54.83|\n",
      "|2015-01-11|    null|              9.94|\n",
      "|2015-01-13|    null|3553.7949999999996|\n",
      "|2015-01-14|    null|             61.96|\n",
      "|2015-01-15|    null|            149.95|\n",
      "|2015-01-16|    null|           299.964|\n",
      "|2015-01-18|    null|            64.864|\n",
      "|2015-01-19|    null|378.59400000000005|\n",
      "|2015-01-20|    null|           2673.87|\n",
      "|2015-01-21|    null|            25.248|\n",
      "|2015-01-23|    null|46.019999999999996|\n",
      "|2015-01-26|    null|           1097.25|\n",
      "|2015-01-27|    null|            426.67|\n",
      "+----------+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rolledUpDF.where(\"Category IS NULL\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+------------------+\n",
      "|date|Category|       total_sales|\n",
      "+----+--------+------------------+\n",
      "|null|    null|2297200.8603000017|\n",
      "+----+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rolledUpDF.where(\"date IS NULL\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cube function\n",
    "\n",
    "cubeDF = dfNoNull.cube(\"date\", \"Category\", \"Sales\")\\\n",
    ".agg(max(\"Sales\"))\\\n",
    ".selectExpr(\"date\", \"Category\", \"`max(Sales)` as highest_sales\")\\\n",
    ".orderBy(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+------------------+\n",
      "|      date|       Category|     highest_sales|\n",
      "+----------+---------------+------------------+\n",
      "|2018-12-30|     Technology|             90.93|\n",
      "|2018-12-30|Office Supplies|             3.024|\n",
      "|2018-12-30|Office Supplies|             209.3|\n",
      "|2018-12-30|           null|             90.93|\n",
      "|2018-12-30|           null|            13.904|\n",
      "|2018-12-30|           null|           323.136|\n",
      "|2018-12-30|Office Supplies|            13.904|\n",
      "|2018-12-30|           null|             20.72|\n",
      "|2018-12-30|Office Supplies|             20.72|\n",
      "|2018-12-30|           null|             3.024|\n",
      "|2018-12-30|           null|             209.3|\n",
      "|2018-12-30|      Furniture|           323.136|\n",
      "|2018-12-30|      Furniture|           323.136|\n",
      "|2018-12-30|Office Supplies|52.775999999999996|\n",
      "|2018-12-30|           null|52.775999999999996|\n",
      "|2018-12-30|     Technology|             90.93|\n",
      "|2018-12-30|           null|           323.136|\n",
      "|2018-12-30|Office Supplies|             209.3|\n",
      "|2018-12-29|           null|           393.568|\n",
      "|2018-12-29|           null|            300.98|\n",
      "+----------+---------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cubeDF.sort(\"date\", ascending = False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+------------------+\n",
      "|date|Category|     highest_sales|\n",
      "+----+--------+------------------+\n",
      "|null|    null|2.8160000000000003|\n",
      "|null|    null|              7.76|\n",
      "|null|    null|              8.62|\n",
      "|null|    null|            72.784|\n",
      "|null|    null|374.37600000000003|\n",
      "|null|    null|             47.82|\n",
      "|null|    null|25.920000000000005|\n",
      "|null|    null|             37.32|\n",
      "|null|    null|             43.68|\n",
      "|null|    null|            35.712|\n",
      "|null|    null|              10.9|\n",
      "|null|    null|           170.058|\n",
      "|null|    null|333.09000000000003|\n",
      "|null|    null| 56.82000000000001|\n",
      "|null|    null|            95.992|\n",
      "|null|    null|15.959999999999999|\n",
      "|null|    null|435.16800000000006|\n",
      "|null|    null|25.695999999999998|\n",
      "|null|    null|            141.42|\n",
      "|null|    null|           151.188|\n",
      "+----+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cubeDF.where(\"Category IS NULL\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivotDF = dfWithDate.groupby(\"date\").pivot(\"Category\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+\n",
      "|      date|         Furniture|   Office Supplies|        Technology|\n",
      "+----------+------------------+------------------+------------------+\n",
      "|2018-12-01|          2542.292|1371.4499999999998|1417.4360000000001|\n",
      "|2018-12-02|          3527.504|          2665.612|3758.0660000000003|\n",
      "|2018-12-03|            591.84|           447.932|364.07000000000005|\n",
      "|2018-12-04| 992.7819999999999|           199.138|          1447.718|\n",
      "|2018-12-05|            321.48|            796.06|           335.596|\n",
      "|2018-12-06|              null|             10.68|              null|\n",
      "|2018-12-07|             82.38|          2279.774|            554.36|\n",
      "|2018-12-08|3368.2650000000003|2632.7520000000004|1642.0240000000001|\n",
      "|2018-12-09|1927.2330000000002|          1447.079|          2096.078|\n",
      "|2018-12-10|          2102.264|           973.235| 798.0600000000002|\n",
      "|2018-12-11|          1448.529| 899.7460000000001|475.69000000000005|\n",
      "|2018-12-13|489.01599999999996|             91.92|              null|\n",
      "|2018-12-14|1852.5319999999997|320.73199999999997|           1724.45|\n",
      "|2018-12-15|             22.77| 87.34200000000001|           196.776|\n",
      "|2018-12-16|197.71200000000002|321.09000000000003|             339.9|\n",
      "|2018-12-17|            629.26|1398.4979999999998|              null|\n",
      "|2018-12-18|          1380.681|           2115.91|            149.32|\n",
      "|2018-12-19|115.37800000000001|          1780.548|              null|\n",
      "|2018-12-20|              null|377.73599999999993|              null|\n",
      "|2018-12-21|             15.92|1772.0500000000002|352.96999999999997|\n",
      "|2018-12-22| 4086.455999999999|          1851.813|          1503.752|\n",
      "|2018-12-23|           282.114|          1149.692|            494.97|\n",
      "|2018-12-24|1393.4940000000001|1479.6380000000001|3359.9220000000005|\n",
      "|2018-12-25|           832.454|1465.2649999999999|           401.208|\n",
      "|2018-12-26|              null| 814.5939999999999|              null|\n",
      "|2018-12-27|              null|            13.248|           164.388|\n",
      "|2018-12-28| 551.2568000000001|1091.2440000000001|14.850000000000001|\n",
      "|2018-12-29|          2330.718|            282.44|           302.376|\n",
      "|2018-12-30|           323.136|299.72400000000005|             90.93|\n",
      "+----------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivotDF.where(\"`date` > '2018-11-30'\").orderBy(\"date\", ascending = True).show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id, col, year\n",
    "\n",
    "df1 = superstore.select(monotonically_increasing_id(), year(\"Order Date\"))\\\n",
    ".withColumnRenamed(\"year(Order Date)\", \"date1\")\\\n",
    ".withColumnRenamed(\"monotonically_increasing_id()\", \"ROW_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = superstore.select(monotonically_increasing_id(), year(\"Ship Date\"))\\\n",
    ".withColumnRenamed(\"year(Ship Date)\", \"date2\")\\\n",
    ".withColumnRenamed(\"monotonically_increasing_id()\", \"ROW_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.join(df1, df1.ROW_ID == df2.ROW_ID)\\\n",
    ".withColumn(\"difference\", df2.date2 - df1.date1)\\\n",
    ".selectExpr(\"date1\", \"date2\", \"difference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----------+\n",
      "|date1|date2|difference|\n",
      "+-----+-----+----------+\n",
      "| 2017| 2017|         0|\n",
      "| 2016| 2016|         0|\n",
      "| 2015| 2015|         0|\n",
      "| 2018| 2018|         0|\n",
      "| 2017| 2017|         0|\n",
      "| 2016| 2016|         0|\n",
      "| 2017| 2017|         0|\n",
      "| 2018| 2018|         0|\n",
      "| 2016| 2016|         0|\n",
      "| 2016| 2016|         0|\n",
      "| 2017| 2017|         0|\n",
      "| 2018| 2018|         0|\n",
      "| 2018| 2018|         0|\n",
      "| 2016| 2016|         0|\n",
      "| 2017| 2017|         0|\n",
      "+-----+-----+----------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----------+\n",
      "|date1|date2|difference|\n",
      "+-----+-----+----------+\n",
      "| 2018| 2019|         1|\n",
      "| 2018| 2019|         1|\n",
      "| 2018| 2019|         1|\n",
      "| 2018| 2019|         1|\n",
      "| 2018| 2019|         1|\n",
      "| 2018| 2019|         1|\n",
      "| 2018| 2019|         1|\n",
      "| 2018| 2019|         1|\n",
      "| 2018| 2019|         1|\n",
      "| 2018| 2019|         1|\n",
      "| 2018| 2019|         1|\n",
      "| 2018| 2019|         1|\n",
      "| 2018| 2019|         1|\n",
      "| 2018| 2019|         1|\n",
      "| 2018| 2019|         1|\n",
      "| 2018| 2019|         1|\n",
      "| 2018| 2019|         1|\n",
      "| 2018| 2019|         1|\n",
      "| 2018| 2019|         1|\n",
      "| 2018| 2019|         1|\n",
      "+-----+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.where(\"difference > 0\").orderBy(\"date1\", ascending = False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>summary</th>\n",
       "      <td>count</td>\n",
       "      <td>mean</td>\n",
       "      <td>stddev</td>\n",
       "      <td>min</td>\n",
       "      <td>max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date1</th>\n",
       "      <td>9994</td>\n",
       "      <td>2016.722233340004</td>\n",
       "      <td>1.1235549110443188</td>\n",
       "      <td>2015</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date2</th>\n",
       "      <td>9994</td>\n",
       "      <td>2016.7374424654793</td>\n",
       "      <td>1.1261405941480775</td>\n",
       "      <td>2015</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>difference</th>\n",
       "      <td>9994</td>\n",
       "      <td>0.015209125475285171</td>\n",
       "      <td>0.12238997837870935</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0                     1                    2     3     4\n",
       "summary     count                  mean               stddev   min   max\n",
       "date1        9994     2016.722233340004   1.1235549110443188  2015  2018\n",
       "date2        9994    2016.7374424654793   1.1261405941480775  2015  2019\n",
       "difference   9994  0.015209125475285171  0.12238997837870935     0     1"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.describe().toPandas().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pair RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pair RDD with tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://CGSCLRD42045501.in623.corpintra.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1eb9392d208>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "person = spark.createDataFrame([\n",
    "    (0, \"Bill Chambers\", 0, [100]),\n",
    "    (1, \"Matei Zaharia\", 1, [500, 250, 100]),\n",
    "    (2, \"Michael Armbrust\", 1, [250, 100])]).toDF(\"id\", \"name\", \"graduate_course\", \"spark_status\")\n",
    "\n",
    "graduateProgram = spark.createDataFrame([\n",
    "    (0, \"Masters\", \"School of Information\", \"UC Berkeley\"),\n",
    "    (2, \"Masters\", \"EECS\", \"UC Berkeley\"),\n",
    "    (1, \"Ph.D.\", \"EECS\", \"UC Berkeley\")]).toDF(\"id\", \"degree\", \"department\", \"school\")\n",
    "\n",
    "sparkStatus = spark.createDataFrame([\n",
    "    (500, \"Vice President\"),\n",
    "    (250, \"PMC Member\"),\n",
    "    (100, \"Contributor\")]).toDF(\"id\", \"status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "person.createOrReplaceTempView(\"person\")\n",
    "graduateProgram.createOrReplaceTempView(\"graduateProgram\")\n",
    "sparkStatus.createOrReplaceTempView(\"sparkStatus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inner Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+---------------+---------------+---+-------+--------------------+-----------+\n",
      "| id|            name|graduate_course|   spark_status| id| degree|          department|     school|\n",
      "+---+----------------+---------------+---------------+---+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|              0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|   Matei Zaharia|              1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  2|Michael Armbrust|              1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+----------------+---------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinCondition = person[\"graduate_course\"] == graduateProgram[\"id\"]\n",
    "\n",
    "joinedDf = person.join(graduateProgram, joinCondition)\n",
    "joinedDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+---------------+---------------+---+-------+--------------------+-----------+\n",
      "| id|            name|graduate_course|   spark_status| id| degree|          department|     school|\n",
      "+---+----------------+---------------+---------------+---+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|              0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|   Matei Zaharia|              1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  2|Michael Armbrust|              1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+----------------+---------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM person INNER JOIN graduateProgram ON person.graduate_course = graduateProgram.id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outer Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+---------------+---------------+---+-------+--------------------+-----------+\n",
      "|  id|            name|graduate_course|   spark_status| id| degree|          department|     school|\n",
      "+----+----------------+---------------+---------------+---+-------+--------------------+-----------+\n",
      "|   0|   Bill Chambers|              0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|   1|   Matei Zaharia|              1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|   2|Michael Armbrust|              1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|null|            null|           null|           null|  2|Masters|                EECS|UC Berkeley|\n",
      "+----+----------------+---------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinType = \"outer\"\n",
    "\n",
    "joinedDf = person.join(graduateProgram, joinCondition, joinType)\n",
    "joinedDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+---------------+---------------+---+-------+--------------------+-----------+\n",
      "|  id|            name|graduate_course|   spark_status| id| degree|          department|     school|\n",
      "+----+----------------+---------------+---------------+---+-------+--------------------+-----------+\n",
      "|   0|   Bill Chambers|              0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|   1|   Matei Zaharia|              1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|   2|Michael Armbrust|              1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|null|            null|           null|           null|  2|Masters|                EECS|UC Berkeley|\n",
      "+----+----------------+---------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM person FULL OUTER JOIN graduateProgram ON person.graduate_course = graduateProgram.id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left Outer Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+---------------+---------------+---+-------+--------------------+-----------+\n",
      "| id|            name|graduate_course|   spark_status| id| degree|          department|     school|\n",
      "+---+----------------+---------------+---------------+---+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|              0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|   Matei Zaharia|              1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  2|Michael Armbrust|              1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+----------------+---------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinType = \"left_outer\"\n",
    "\n",
    "joinedDf = person.join(graduateProgram, joinCondition, joinType)\n",
    "joinedDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+---------------+---------------+---+-------+--------------------+-----------+\n",
      "| id|            name|graduate_course|   spark_status| id| degree|          department|     school|\n",
      "+---+----------------+---------------+---------------+---+-------+--------------------+-----------+\n",
      "|  0|   Bill Chambers|              0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|   Matei Zaharia|              1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|  2|Michael Armbrust|              1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+----------------+---------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM person LEFT OUTER JOIN graduateProgram ON person.graduate_course = graduateProgram.id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Right Outer Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+---------------+---------------+---+-------+--------------------+-----------+\n",
      "|  id|            name|graduate_course|   spark_status| id| degree|          department|     school|\n",
      "+----+----------------+---------------+---------------+---+-------+--------------------+-----------+\n",
      "|   0|   Bill Chambers|              0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|   1|   Matei Zaharia|              1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|   2|Michael Armbrust|              1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|null|            null|           null|           null|  2|Masters|                EECS|UC Berkeley|\n",
      "+----+----------------+---------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinType = \"right_outer\"\n",
    "\n",
    "joinedDf = person.join(graduateProgram, joinCondition, joinType)\n",
    "joinedDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+---------------+---------------+---+-------+--------------------+-----------+\n",
      "|  id|            name|graduate_course|   spark_status| id| degree|          department|     school|\n",
      "+----+----------------+---------------+---------------+---+-------+--------------------+-----------+\n",
      "|   0|   Bill Chambers|              0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|   1|   Matei Zaharia|              1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|   2|Michael Armbrust|              1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "|null|            null|           null|           null|  2|Masters|                EECS|UC Berkeley|\n",
      "+----+----------------+---------------+---------------+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM person RIGHT OUTER JOIN graduateProgram ON person.graduate_course = graduateProgram.id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left Semi Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------+\n",
      "| id| degree|          department|     school|\n",
      "+---+-------+--------------------+-----------+\n",
      "|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinType = \"left_semi\"\n",
    "\n",
    "joinedDf = graduateProgram.join(person, joinCondition, joinType)\n",
    "joinedDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-----------+\n",
      "| id| degree|          department|     school|\n",
      "+---+-------+--------------------+-----------+\n",
      "|  0|Masters|School of Informa...|UC Berkeley|\n",
      "|  1|  Ph.D.|                EECS|UC Berkeley|\n",
      "+---+-------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM graduateProgram LEFT SEMI JOIN person ON graduateProgram.id = person.graduate_course\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left Anti Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+-----------+\n",
      "| id| degree|department|     school|\n",
      "+---+-------+----------+-----------+\n",
      "|  2|Masters|      EECS|UC Berkeley|\n",
      "+---+-------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinType = \"left_anti\"\n",
    "\n",
    "joinedDf = graduateProgram.join(person, joinCondition, joinType)\n",
    "joinedDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+-----------+\n",
      "| id| degree|department|     school|\n",
      "+---+-------+----------+-----------+\n",
      "|  2|Masters|      EECS|UC Berkeley|\n",
      "+---+-------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM graduateProgram LEFT ANTI JOIN person ON graduateProgram.id = person.graduate_course\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "joinType = \"cross\"\n",
    "\n",
    "joinedDf = person.join(graduateProgram, joinCondition, joinType)\n",
    "joinedDf.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "spark.sql(\"SELECT * FROM person CROSS JOIN graduateProgram\").show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window Functions and Sub-Queries in Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+-------------------+-------------------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+------------------+--------+--------+-------------------+\n",
      "|Row ID|      Order ID|         Order Date|          Ship Date|     Ship Mode|Customer ID|  Customer Name|  Segment|      Country|           City|     State|Postal Code|Region|     Product ID|       Category|Sub-Category|        Product Name|             Sales|Quantity|Discount|             Profit|\n",
      "+------+--------------+-------------------+-------------------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+------------------+--------+--------+-------------------+\n",
      "|     1|CA-2017-152156|2017-11-08 00:00:00|2017-11-11 00:00:00|  Second Class|   CG-12520|    Claire Gute| Consumer|United States|      Henderson|  Kentucky|    42420.0| South|FUR-BO-10001798|      Furniture|   Bookcases|Bush Somerset Col...|            261.96|       2|     0.0|            41.9136|\n",
      "|     2|CA-2017-152156|2017-11-08 00:00:00|2017-11-11 00:00:00|  Second Class|   CG-12520|    Claire Gute| Consumer|United States|      Henderson|  Kentucky|    42420.0| South|FUR-CH-10000454|      Furniture|      Chairs|Hon Deluxe Fabric...| 731.9399999999999|       3|     0.0| 219.58199999999997|\n",
      "|     3|CA-2017-138688|2017-06-12 00:00:00|2017-06-16 00:00:00|  Second Class|   DV-13045|Darrin Van Huff|Corporate|United States|    Los Angeles|California|    90036.0|  West|OFF-LA-10000240|Office Supplies|      Labels|Self-Adhesive Add...|             14.62|       2|     0.0| 6.8713999999999995|\n",
      "|     4|US-2016-108966|2016-10-11 00:00:00|2016-10-18 00:00:00|Standard Class|   SO-20335| Sean O'Donnell| Consumer|United States|Fort Lauderdale|   Florida|    33311.0| South|FUR-TA-10000577|      Furniture|      Tables|Bretford CR4500 S...|          957.5775|       5|    0.45|-383.03100000000006|\n",
      "|     5|US-2016-108966|2016-10-11 00:00:00|2016-10-18 00:00:00|Standard Class|   SO-20335| Sean O'Donnell| Consumer|United States|Fort Lauderdale|   Florida|    33311.0| South|OFF-ST-10000760|Office Supplies|     Storage|Eldon Fold 'N Rol...|22.368000000000002|       2|     0.2|  2.516399999999999|\n",
      "+------+--------------+-------------------+-------------------+--------------+-----------+---------------+---------+-------------+---------------+----------+-----------+------+---------------+---------------+------------+--------------------+------------------+--------+--------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM `superstore_view` LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------------+------------------+------------------+------------------+----------+\n",
      "|   Customer Name|         Order Date|             Sales|         Max_Sales|         Min_Sales| Avg_Sales|\n",
      "+----------------+-------------------+------------------+------------------+------------------+----------+\n",
      "|   Brendan Sweed|2015-12-05 00:00:00|1113.0240000000001|1113.0240000000001|              15.0|185.227625|\n",
      "|   Brendan Sweed|2015-12-05 00:00:00|167.96800000000002|1113.0240000000001|              15.0|185.227625|\n",
      "|  David Kendrick|2015-12-05 00:00:00|24.816000000000003|1113.0240000000001|              15.0|185.227625|\n",
      "|  David Kendrick|2015-12-05 00:00:00|408.74399999999997|1113.0240000000001|              15.0|185.227625|\n",
      "|     Roy Collins|2015-12-05 00:00:00|             24.56|1113.0240000000001|              15.0|185.227625|\n",
      "|       Nona Balk|2015-12-05 00:00:00|           348.488|1113.0240000000001|              15.0|185.227625|\n",
      "|       Nona Balk|2015-12-05 00:00:00|           172.736|1113.0240000000001|              15.0|185.227625|\n",
      "|   Erin Ashbrook|2015-12-05 00:00:00|            98.376|1113.0240000000001|              15.0|185.227625|\n",
      "|   Erin Ashbrook|2015-12-05 00:00:00|29.940000000000005|1113.0240000000001|              15.0|185.227625|\n",
      "|   Erin Ashbrook|2015-12-05 00:00:00|            17.472|1113.0240000000001|              15.0|185.227625|\n",
      "|   Erin Ashbrook|2015-12-05 00:00:00|            36.738|1113.0240000000001|              15.0|185.227625|\n",
      "|   Erin Ashbrook|2015-12-05 00:00:00|179.93999999999997|1113.0240000000001|              15.0|185.227625|\n",
      "|       Rob Lucas|2015-12-05 00:00:00|             26.46|1113.0240000000001|              15.0|185.227625|\n",
      "|       Rob Lucas|2015-12-05 00:00:00|             49.12|1113.0240000000001|              15.0|185.227625|\n",
      "|       Rob Lucas|2015-12-05 00:00:00|              15.0|1113.0240000000001|              15.0|185.227625|\n",
      "|  Joni Blumstein|2015-12-05 00:00:00|            250.26|1113.0240000000001|              15.0|185.227625|\n",
      "|Jennifer Braxton|2016-05-01 00:00:00|            88.752|           172.704|12.176000000000002|     64.42|\n",
      "|    David Bremer|2016-05-01 00:00:00| 63.55200000000001|           172.704|12.176000000000002|     64.42|\n",
      "|    David Bremer|2016-05-01 00:00:00|            41.376|           172.704|12.176000000000002|     64.42|\n",
      "|    David Bremer|2016-05-01 00:00:00|           172.704|           172.704|12.176000000000002|     64.42|\n",
      "+----------------+-------------------+------------------+------------------+------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Maximum, Minimum and Avg Sales Per Order Date\n",
    "\n",
    "spark.sql(\"SELECT `Customer Name`, `Order Date`, Sales, \\\n",
    "          MAX(Sales) OVER(PARTITION BY `Order Date`) as Max_Sales, \\\n",
    "          MIN(Sales) OVER(PARTITION BY `Order Date`) as Min_Sales,  \\\n",
    "          AVG(Sales) Over(PARTITION BY `Order Date`) as Avg_Sales \\\n",
    "          FROM superstore_view\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+-------------------+-------------------+-----------------+\n",
      "|    Customer Name|             Sales| Current_Order_Date|    Next_Order_Date|DaysBetweenOrders|\n",
      "+-----------------+------------------+-------------------+-------------------+-----------------+\n",
      "|Jennifer Halladay|22.776000000000003|2015-02-24 00:00:00|2018-11-19 00:00:00|             1364|\n",
      "| Mitch Willingham|            617.97|2015-05-21 00:00:00|2018-11-16 00:00:00|             1275|\n",
      "|    Beth Fritzler|           314.352|2015-03-22 00:00:00|2018-07-10 00:00:00|             1206|\n",
      "|    Hilary Holden|             13.36|2015-08-29 00:00:00|2018-10-19 00:00:00|             1147|\n",
      "|   David Kendrick|408.74399999999997|2015-12-05 00:00:00|2018-12-20 00:00:00|             1111|\n",
      "|    Randy Bradley|            657.93|2015-11-17 00:00:00|2018-10-23 00:00:00|             1071|\n",
      "|   Sharelle Roach|            212.94|2015-06-14 00:00:00|2018-04-17 00:00:00|             1038|\n",
      "|   Vicky Freymann|             15.52|2015-07-25 00:00:00|2018-05-22 00:00:00|             1032|\n",
      "|     Astrea Jones|            35.448|2015-12-30 00:00:00|2018-10-17 00:00:00|             1022|\n",
      "|        Bart Folk|14.669999999999998|2015-11-28 00:00:00|2018-09-12 00:00:00|             1019|\n",
      "|     Mathew Reese|            129.92|2015-10-07 00:00:00|2018-07-08 00:00:00|             1005|\n",
      "|       Eva Jacobs| 626.3520000000001|2015-03-03 00:00:00|2017-12-01 00:00:00|             1004|\n",
      "|       Sarah Bern|13.984000000000002|2015-06-06 00:00:00|2018-02-09 00:00:00|              979|\n",
      "|    Aaron Bergman|            242.94|2015-03-07 00:00:00|2017-11-10 00:00:00|              979|\n",
      "| Scott Williamson|             22.38|2015-03-10 00:00:00|2017-11-10 00:00:00|              976|\n",
      "|  Frank Gastineau|             34.44|2015-11-17 00:00:00|2018-07-10 00:00:00|              966|\n",
      "|    Ionia McGrath|             40.08|2015-01-23 00:00:00|2017-09-04 00:00:00|              955|\n",
      "|  Jasper Cacioppo|3.9280000000000004|2015-01-28 00:00:00|2017-09-05 00:00:00|              951|\n",
      "|     Larry Hughes| 6.570000000000001|2015-09-21 00:00:00|2018-04-25 00:00:00|              947|\n",
      "|       Larry Tron|           201.584|2016-05-12 00:00:00|2018-12-04 00:00:00|              936|\n",
      "+-----------------+------------------+-------------------+-------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Days between orders for each customer sorted by descending order of days between orders\n",
    "\n",
    "spark.sql(\"SELECT `Customer Name`, Sales, Current_Order_Date, Next_Order_Date, \\\n",
    "          DATEDIFF(Next_Order_Date, Current_Order_Date) as DaysBetweenOrders \\\n",
    "          FROM \\\n",
    "          \\\n",
    "          (SELECT `Customer Name`, Sales, `Order Date` as Current_Order_Date, \\\n",
    "          LEAD(`Order Date`, 1) OVER(PARTITION BY `Customer Name` ORDER BY `Order Date`) as Next_Order_Date \\\n",
    "          FROM superstore_view) \\\n",
    "          \\\n",
    "          ORDER BY DaysBetweenOrders DESC\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+------------------+---------------------+\n",
      "|           City|       Customer Name|       Total_sales|customer_rank_in_city|\n",
      "+---------------+--------------------+------------------+---------------------+\n",
      "|   Jacksonville|         Sean Miller|         23661.228|                    1|\n",
      "|      Lafayette|        Tamara Chand|18336.739999999994|                    1|\n",
      "|        Seattle|        Raymond Buch|          14052.48|                    1|\n",
      "|  New York City|        Tom Ashbrook|13723.498000000001|                    1|\n",
      "|    San Antonio|        Becky Martin|10539.895999999999|                    1|\n",
      "|         Newark|        Hunter Lopez|          10499.97|                    1|\n",
      "|    Minneapolis|        Sanjit Chand|           9900.19|                    1|\n",
      "|        Detroit|       Adrian Barton|           9892.74|                    1|\n",
      "|       Lakewood|        Bill Shonely| 9135.189999999999|                    1|\n",
      "|      Arlington|        Sanjit Engle|           8805.04|                    1|\n",
      "|   Philadelphia|  Christopher Conant| 8539.019999999999|                    1|\n",
      "|  San Francisco|        Ken Lonsdale| 8319.289999999999|                    1|\n",
      "|     Burlington|      Grant Thornton| 8167.419999999999|                    1|\n",
      "|        Atlanta|Christopher Martinez|6412.7699999999995|                    1|\n",
      "|        Buffalo|           Greg Tran| 6160.616000000001|                    1|\n",
      "|        Yonkers|       Karen Daniels|          6125.822|                    1|\n",
      "|        Jackson|         Andy Reiter|            5802.7|                    1|\n",
      "|        Houston|        Sean Braxton|           5579.94|                    1|\n",
      "|     Providence|       Daniel Raglin|           5549.41|                    1|\n",
      "|     Sacramento|           Jane Waco|          5325.884|                    1|\n",
      "|      Lancaster|       Cindy Stewart|          5016.549|                    1|\n",
      "|    Springfield|    Kristen Hastings| 4984.829000000001|                    1|\n",
      "|    Los Angeles|       Robert Marley|          4881.132|                    1|\n",
      "|      San Diego|           Max Jones|           4823.09|                    1|\n",
      "|North Las Vegas|       Dennis Pardue| 4685.076000000001|                    1|\n",
      "|         Fresno|          Nora Preis|         4630.5105|                    1|\n",
      "|    Tallahassee|     Patrick O'Brill| 4590.344000000001|                    1|\n",
      "|      Jamestown|         John Murray|           4548.81|                    1|\n",
      "|   Indianapolis|     Helen Wasserman| 4472.320000000001|                    1|\n",
      "|     Round Rock|          Alex Avila|          4406.072|                    1|\n",
      "|      Henderson|       Maria Etezadi|           4374.88|                    1|\n",
      "|        Concord|         Joseph Holt|          4297.644|                    1|\n",
      "|     Alexandria|        Greg Maxwell| 4251.919999999999|                    1|\n",
      "|    Great Falls|      Alan Dominguez| 4189.379999999999|                    1|\n",
      "|        Midland|     Shirley Daniels|          3999.702|                    1|\n",
      "|       Florence|     Katrina Willman|3747.9300000000003|                    1|\n",
      "|      Lakeville|       Dianna Wilson|           3745.63|                    1|\n",
      "|       Columbia|   Giulietta Baptist|           3631.96|                    1|\n",
      "|          Parma|        Susan Pistek|          3588.096|                    1|\n",
      "|  Santa Barbara|       Maria Etezadi|3528.4880000000003|                    1|\n",
      "|      Brentwood|         Rick Wilson|3397.7319999999995|                    1|\n",
      "| Virginia Beach|    Mitch Willingham|3333.9000000000005|                    1|\n",
      "|          Dover|         Peter McVee|          3332.974|                    1|\n",
      "|        Burbank|    Sample Company A|3247.1580000000004|                    1|\n",
      "|         Mobile|        Mark Cousins|           3236.41|                    1|\n",
      "|        Chicago|           Rob Lucas|          3172.707|                    1|\n",
      "|       Lakeland|             Jim Epp|          3165.744|                    1|\n",
      "|       Richmond|       Tonja Turnell|           3083.94|                    1|\n",
      "|       Columbus|     Bradley Drucker|           3002.65|                    1|\n",
      "|      Inglewood|     Dan Reichenbach|2971.7920000000004|                    1|\n",
      "+---------------+--------------------+------------------+---------------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Top Customers in each city by total sales\n",
    "\n",
    "spark.sql(\"SELECT City, `Customer Name`, Total_sales, \\\n",
    "          ROW_NUMBER() OVER(PARTITION BY CITY ORDER BY Total_sales DESC) as customer_rank_in_city \\\n",
    "          FROM \\\n",
    "          \\\n",
    "          (SELECT City, `Customer Name`, SUM(Sales) as Total_sales  \\\n",
    "          FROM superstore_view \\\n",
    "          GROUP BY `Customer Name`, City \\\n",
    "          ORDER BY City) \\\n",
    "          \\\n",
    "          ORDER BY Total_sales DESC\").where(col(\"customer_rank_in_city\") == 1).show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+------------------+------------------+----------+-----------+---------+\n",
      "|    Segment|        Product Name|       Total_sales|      Total_profit|sales_rank|profit_rank|rank_diff|\n",
      "+-----------+--------------------+------------------+------------------+----------+-----------+---------+\n",
      "|   Consumer|Canon imageCLASS ...|32899.905999999995|12879.963199999998|         1|          1|        0|\n",
      "|   Consumer|Ibico EPK-21 Elec...|           9449.95|4630.4755000000005|        12|          2|       10|\n",
      "|   Consumer|HP Designjet T520...|         18374.895|4094.9765999999986|         2|          3|       -1|\n",
      "|   Consumer|Ativa V4110MDD Mi...|           7699.89|3772.9460999999997|        15|          4|       11|\n",
      "|   Consumer|Fellowes PB500 El...|         14489.286|          3685.871|         5|          5|        0|\n",
      "|   Consumer|GBC Ibimaster 500...|          17122.05|2663.4300000000003|         4|          6|       -2|\n",
      "|   Consumer|Canon PC1060 Pers...|          6719.904|2267.9675999999995|        18|          7|       11|\n",
      "|   Consumer|Ibico Ibimaster 3...|          5961.438|2115.9424999999997|        25|          8|       17|\n",
      "|   Consumer|Sharp AL-1530CS D...| 6399.871999999999|         2064.9587|        20|          9|       11|\n",
      "|   Consumer|Plantronics CS510...|           6071.08|1870.8164999999995|        22|         10|       12|\n",
      "|  Corporate|Canon imageCLASS ...|17499.949999999997| 8399.975999999999|         1|          1|        0|\n",
      "|  Corporate|3D Systems Cube P...|          14299.89|3717.9713999999994|         2|          2|        0|\n",
      "|  Corporate|Fellowes PB300 Pl...| 7759.800000000001|3030.2019000000005|         5|          3|        2|\n",
      "|  Corporate|Fellowes PB500 El...|          7117.544|2669.0789999999997|         6|          4|        2|\n",
      "|  Corporate|Canon PC1060 Pers...|           4899.93|         2302.9671|        14|          5|        9|\n",
      "|  Corporate|Hewlett Packard L...|           5399.91|2279.9620000000004|        11|          6|        5|\n",
      "|  Corporate|Zebra ZM400 Therm...|            4643.8|          2229.024|        18|          7|       11|\n",
      "|  Corporate|GBC DocuBind TL30...| 6727.425000000001|1497.9732999999994|         7|          8|       -1|\n",
      "|  Corporate|Honeywell Envirac...|           4509.75|1488.2174999999997|        19|          9|       10|\n",
      "|  Corporate|  Canon PC940 Copier|3149.9300000000003|1480.4670999999998|        47|         10|       37|\n",
      "|Home Office|Hewlett Packard L...|          9239.846|         3935.9344|         3|          1|        2|\n",
      "|Home Office|Canon imageCLASS ...|         11199.968| 3919.988799999999|         2|          2|        0|\n",
      "|Home Office|Canon imageCLASS ...|           3991.98|           1995.99|        11|          3|        8|\n",
      "|Home Office|Hewlett-Packard D...|            3404.5|          1668.205|        16|          4|       12|\n",
      "|Home Office|Ibico EPK-21 Elec...| 4535.976000000001|1644.2912999999999|         9|          5|        4|\n",
      "|Home Office|Global Deluxe Hig...|           7006.51|1524.2733999999994|         4|          6|       -2|\n",
      "|Home Office|Plantronics Savi ...|            3375.6|1485.2640000000001|        18|          7|       11|\n",
      "|Home Office|Fellowes PB500 El...|          5846.554|          1398.089|         6|          8|       -2|\n",
      "|Home Office|Canon Image Class...|           2999.95|1379.9769999999999|        26|          9|       17|\n",
      "|Home Office|Zebra ZM400 Therm...|            2321.9|          1114.512|        39|         10|       29|\n",
      "+-----------+--------------------+------------------+------------------+----------+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Rank the top 10 products based on profit in each segment sorted from 1 to 10 under profit_rank\n",
    "#Rank the products in each segment based on sales under a sales_rank column\n",
    "#Find the difference in sales_ranks & profit_ranks for each product\n",
    "\n",
    "spark.sql(\"SELECT *, (sales_rank - profit_rank) as rank_diff \\\n",
    "          FROM \\\n",
    "          \\\n",
    "              (SELECT Segment, `Product Name`,Total_sales, Total_profit, \\\n",
    "              ROW_NUMBER() OVER(PARTITION BY Segment ORDER BY Total_sales DESC) as sales_rank, \\\n",
    "              ROW_NUMBER() OVER(PARTITION BY Segment ORDER BY Total_profit DESC) as profit_rank \\\n",
    "              FROM \\\n",
    "          \\\n",
    "                  (SELECT Segment, `Product Name`, \\\n",
    "                  SUM(Sales) as Total_sales, \\\n",
    "                  SUM(Profit) as Total_profit  \\\n",
    "                  FROM superstore_view \\\n",
    "                  GROUP BY Segment, `Product Name` \\\n",
    "                  ORDER BY Segment) \\\n",
    "          \\\n",
    "          ORDER BY Segment, Total_profit DESC) \\\n",
    "          WHERE profit_rank <= 10\").show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
